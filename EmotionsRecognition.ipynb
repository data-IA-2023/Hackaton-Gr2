{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CjWvnaQUrZmD"
      },
      "source": [
        "# Emotion classification using the RAVDESS dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ldtHMhuLrewK"
      },
      "source": [
        "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) is licensed under CC BY-NA-SC 4.0. and can be downloaded free of charge at https://zenodo.org/record/1188976.\n",
        "\n",
        "***Construction and Validation***\n",
        "\n",
        "Construction and validation of the RAVDESS is described in our paper: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
        "\n",
        "The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLOS ONE.\n",
        "\n",
        "***Description***\n",
        "\n",
        "The dataset contains the complete set of 7356 RAVDESS files (total size: 24.8 GB). Each of the 24 actors consists of three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.\n",
        "\n",
        "***Data***\n",
        "\n",
        "For this task, I have used 4948 samples from the RAVDESS dataset.\n",
        "\n",
        "The samples comes from:\n",
        "\n",
        "- Audio-only files;\n",
        "- Video + audio files: I have extracted the audio from each file using the script Mp4ToWav.py that you can find in the main directory of the project.\n",
        "\n",
        "***License information***\n",
        "\n",
        "The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NA-SC 4.0\n",
        "\n",
        "***File naming convention***\n",
        "\n",
        "Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:\n",
        "\n",
        "***Filename identifiers***\n",
        "\n",
        "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "- Vocal channel (01 = speech, 02 = song).\n",
        "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
        "- Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
        "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "\n",
        "Filename example: 02-01-06-01-02-01-12.mp4 \n",
        "\n",
        "- Video-only (02)\n",
        "- Speech (01)\n",
        "- Fearful (06)\n",
        "- Normal intensity (01)\n",
        "- Statement “dogs” (02)\n",
        "- 1st Repetition (01)\n",
        "- 12th Actor (12)\n",
        "- Female, as the actor ID number is even."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JDNbxj45rkvB"
      },
      "source": [
        "# Analysis\n",
        "\n",
        "We are using Colab, a Google Cloud environment for jupyter, so we need to import our files from Google Drive and then install LibROSA, a python package for music and audio analysis.\n",
        "\n",
        "After the import, we will plot the signal of the first file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rxI4xzngdS-e"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "from librosa import display\n",
        "\n",
        "data, sampling_rate = librosa.load('/Users/home/Documents/Python/Hackaton-Gr2/static/Ravdess/Actor_01/03-01-01-01-01-01-01.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "colab_type": "code",
        "id": "WgaSHtCIdtX2",
        "outputId": "d6474d00-de9a-4384-d1ba-f2a722bb3cd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 0 Axes>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vCtNuVWlr5jL"
      },
      "source": [
        "# Load all files\n",
        "\n",
        "We will create our numpy array extracting Mel-frequency cepstral coefficients (MFCCs), while the classes to predict will be extracted from the name of the file (see the introductory section of this notebook to see the naming convention of the files of this dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AKvuF--gd6F-"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Users/home/Documents/Python/Hackaton-Gr2/static/RavdessActor_16'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m file_path \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ravdess_directory_list:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# as their are 20 different actors in our previous directory we need to extract files for each actor.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m): actor \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(Ravdess \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mdir\u001b[39m) \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m actor:\n\u001b[1;32m     10\u001b[0m         part \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/home/Documents/Python/Hackaton-Gr2/static/RavdessActor_16'"
          ]
        }
      ],
      "source": [
        "Ravdess = '/Users/home/Documents/Python/Hackaton-Gr2/static/Ravdess/'\n",
        "ravdess_directory_list = os.listdir(Ravdess)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "for dir in ravdess_directory_list:\n",
        "    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n",
        "    if not dir.startswith('.'): actor = os.listdir(Ravdess + dir) \n",
        "    for file in actor:\n",
        "        part = file.split('.')[0]\n",
        "        part = part.split('-')\n",
        "        # third part in each file represents the emotion associated to that file.\n",
        "        file_emotion.append(int(part[2]))\n",
        "        file_path.append(Ravdess + dir + '/' + file)\n",
        "        \n",
        "# dataframe for emotion of files\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "# dataframe for path of files.\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "\n",
        "# changing integers to actual emotions.\n",
        "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
        "Ravdess_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kLSggnF7kKY1"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mlst)\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
          ]
        }
      ],
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "VzvBRTJIlIE9",
        "outputId": "2139450f-efb0-40cf-fea1-be26cea18db1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((4948, 40), (4948,))"
            ]
          },
          "execution_count": 8,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Agw-3KN1sDhh"
      },
      "source": [
        "# Decision Tree Classifier\n",
        "\n",
        "To make a first attempt in accomplishing this classification task I chose a decision tree:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Q-Xgb5NslTBO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UshLOC1ClWL3"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_BnCR52nlXw0"
      },
      "outputs": [],
      "source": [
        "dtree = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "colab_type": "code",
        "id": "qWyTownblZM0",
        "outputId": "130cdfb5-099e-4f4e-c244-75b88ee209f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "            max_features=None, max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
              "            splitter='best')"
            ]
          },
          "execution_count": 12,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dtree.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HEuw6TUQlr7C"
      },
      "outputs": [],
      "source": [
        "predictions = dtree.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_1v0i0V7sMw7"
      },
      "source": [
        "Let's go with our classification report.\n",
        "\n",
        "Before we start, a quick reminder of the classes we are trying to predict:\n",
        "\n",
        "emotions = {\n",
        "    \"neutral\": \"01\",\n",
        "    \"calm\": \"02\",\n",
        "    \"happy\": \"03\",\n",
        "    \"sad\": \"04\",\n",
        "    \"angry\": \"05\", \n",
        "    \"fearful\": \"06\", \n",
        "    \"disgust\": \"07\", \n",
        "    \"surprised\": \"08\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "colab_type": "code",
        "id": "c4kNSYkAleIv",
        "outputId": "40046694-c00d-4543-c51a-2a8058cca2e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          01       0.70      0.72      0.71       134\n",
            "          02       0.85      0.83      0.84       251\n",
            "          03       0.83      0.72      0.77       242\n",
            "          04       0.75      0.72      0.73       271\n",
            "          05       0.84      0.85      0.84       253\n",
            "          06       0.72      0.83      0.77       239\n",
            "          07       0.70      0.71      0.70       127\n",
            "          08       0.75      0.78      0.76       116\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      1633\n",
            "   macro avg       0.77      0.77      0.77      1633\n",
            "weighted avg       0.78      0.78      0.78      1633\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lCVgjLj-gwE2"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jfaTxzZ1w__y"
      },
      "source": [
        "In this second approach, I switched to a random forest classifier and I made a gridsearch to make some hyperparameters tuning.\n",
        "\n",
        "The gridsearch is not shown in the code below otherwise the notebook will require too much time to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wcov_DCXgs7v"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3eo0ljqzg-KM"
      },
      "outputs": [],
      "source": [
        "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
        "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
        "                                 n_estimators= 22000, random_state= 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "colab_type": "code",
        "id": "Tg45qSOfg-26",
        "outputId": "56f1a4e1-aa12-44b7-c281-e0309bb6775f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=3, min_samples_split=20,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=22000, n_jobs=None,\n",
              "            oob_score=False, random_state=5, verbose=0, warm_start=False)"
            ]
          },
          "execution_count": 17,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rforest.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aM8KU3qxhGBM"
      },
      "outputs": [],
      "source": [
        "predictions = rforest.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "colab_type": "code",
        "id": "296FW5sBdanI",
        "outputId": "499a20ac-0d5d-4f0b-91b6-80b1cc4ac9fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          01       1.00      0.54      0.70       134\n",
            "          02       0.66      0.96      0.78       251\n",
            "          03       0.86      0.71      0.78       242\n",
            "          04       0.81      0.64      0.71       271\n",
            "          05       0.89      0.88      0.88       253\n",
            "          06       0.70      0.80      0.75       239\n",
            "          07       0.73      0.61      0.66       127\n",
            "          08       0.60      0.78      0.68       116\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      1633\n",
            "   macro avg       0.78      0.74      0.74      1633\n",
            "weighted avg       0.79      0.76      0.76      1633\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test,predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t9eqMHV3S8i6"
      },
      "source": [
        "# Neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G-QscoyMxQtn"
      },
      "source": [
        "Let's build our neural network!\n",
        "\n",
        "To do so, we need to expand the dimensions of our array, adding a third one using the numpy \"expand_dims\" feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "W4i187-Pe-w5"
      },
      "outputs": [],
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "vnvoCRX1gQCh",
        "outputId": "9ec5dde1-47c6-43ec-bde4-d5cb9260bf75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((3315, 40, 1), (1633, 40, 1))"
            ]
          },
          "execution_count": 21,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_traincnn.shape, x_testcnn.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "HZOGIpuefCd3",
        "outputId": "a6e8069b-21fe-4cbe-df88-eb0e5482833f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LphftMIZzUvz"
      },
      "source": [
        "With *model.summary* we can see a recap of what we have build:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "colab_type": "code",
        "id": "pIWPB4Zgfic7",
        "outputId": "11185b2f-ad63-4afb-e916-7be596faa5d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                6410      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 89,226\n",
            "Trainable params: 89,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5qQSBeBhzcLu"
      },
      "source": [
        "Now we can compile and fit our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iNI1znbsfpTx"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34034
        },
        "colab_type": "code",
        "id": "ktdF-nJKfq6F",
        "outputId": "edd88e8c-8989-4186-ef2d-57d6c6241c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 3315 samples, validate on 1633 samples\n",
            "Epoch 1/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 7.0924 - acc: 0.1430 - val_loss: 2.2302 - val_acc: 0.1972\n",
            "Epoch 2/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 5.2546 - acc: 0.1560 - val_loss: 2.2030 - val_acc: 0.1721\n",
            "Epoch 3/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 4.1979 - acc: 0.1780 - val_loss: 2.7677 - val_acc: 0.2070\n",
            "Epoch 4/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 3.3175 - acc: 0.1900 - val_loss: 2.1432 - val_acc: 0.2266\n",
            "Epoch 5/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 2.7242 - acc: 0.2051 - val_loss: 1.8579 - val_acc: 0.2878\n",
            "Epoch 6/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 2.3140 - acc: 0.2362 - val_loss: 1.8295 - val_acc: 0.3019\n",
            "Epoch 7/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 2.0639 - acc: 0.2434 - val_loss: 1.8248 - val_acc: 0.2694\n",
            "Epoch 8/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.9442 - acc: 0.2627 - val_loss: 1.7866 - val_acc: 0.2266\n",
            "Epoch 9/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 1.9041 - acc: 0.2661 - val_loss: 1.7649 - val_acc: 0.3135\n",
            "Epoch 10/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 1.8269 - acc: 0.3110 - val_loss: 1.7141 - val_acc: 0.3791\n",
            "Epoch 11/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 1.7784 - acc: 0.3164 - val_loss: 1.6804 - val_acc: 0.4011\n",
            "Epoch 12/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.7629 - acc: 0.3261 - val_loss: 1.6833 - val_acc: 0.3717\n",
            "Epoch 13/1000\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 1.7221 - acc: 0.3406 - val_loss: 1.6413 - val_acc: 0.4115\n",
            "Epoch 14/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.7006 - acc: 0.3526 - val_loss: 1.6353 - val_acc: 0.4072\n",
            "Epoch 15/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.6715 - acc: 0.3725 - val_loss: 1.5876 - val_acc: 0.4495\n",
            "Epoch 16/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.6473 - acc: 0.3744 - val_loss: 1.6010 - val_acc: 0.3662\n",
            "Epoch 17/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 1.6256 - acc: 0.3891 - val_loss: 1.5694 - val_acc: 0.4660\n",
            "Epoch 18/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.6071 - acc: 0.4051 - val_loss: 1.5406 - val_acc: 0.4544\n",
            "Epoch 19/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.5914 - acc: 0.4094 - val_loss: 1.5126 - val_acc: 0.4746\n",
            "Epoch 20/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.5710 - acc: 0.4202 - val_loss: 1.5185 - val_acc: 0.4532\n",
            "Epoch 21/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 1.5584 - acc: 0.4193 - val_loss: 1.4906 - val_acc: 0.4685\n",
            "Epoch 22/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.5242 - acc: 0.4344 - val_loss: 1.4660 - val_acc: 0.4813\n",
            "Epoch 23/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 1.5241 - acc: 0.4290 - val_loss: 1.4510 - val_acc: 0.5009\n",
            "Epoch 24/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.5084 - acc: 0.4395 - val_loss: 1.4362 - val_acc: 0.5058\n",
            "Epoch 25/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 1.4914 - acc: 0.4419 - val_loss: 1.4232 - val_acc: 0.5083\n",
            "Epoch 26/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.4719 - acc: 0.4543 - val_loss: 1.4129 - val_acc: 0.4930\n",
            "Epoch 27/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 1.4618 - acc: 0.4588 - val_loss: 1.4072 - val_acc: 0.5168\n",
            "Epoch 28/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.4458 - acc: 0.4627 - val_loss: 1.3825 - val_acc: 0.5236\n",
            "Epoch 29/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.4252 - acc: 0.4697 - val_loss: 1.3814 - val_acc: 0.4936\n",
            "Epoch 30/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.4254 - acc: 0.4736 - val_loss: 1.3757 - val_acc: 0.5083\n",
            "Epoch 31/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.3952 - acc: 0.4917 - val_loss: 1.3392 - val_acc: 0.5340\n",
            "Epoch 32/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.3766 - acc: 0.4968 - val_loss: 1.4141 - val_acc: 0.4746\n",
            "Epoch 33/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.3719 - acc: 0.5077 - val_loss: 1.3195 - val_acc: 0.5242\n",
            "Epoch 34/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 1.3476 - acc: 0.5137 - val_loss: 1.3317 - val_acc: 0.5224\n",
            "Epoch 35/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.3588 - acc: 0.4989 - val_loss: 1.3456 - val_acc: 0.5089\n",
            "Epoch 36/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 1.3468 - acc: 0.5104 - val_loss: 1.3203 - val_acc: 0.5340\n",
            "Epoch 37/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.3364 - acc: 0.4992 - val_loss: 1.3009 - val_acc: 0.5248\n",
            "Epoch 38/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.3264 - acc: 0.5164 - val_loss: 1.2805 - val_acc: 0.5352\n",
            "Epoch 39/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.3259 - acc: 0.5161 - val_loss: 1.2785 - val_acc: 0.5468\n",
            "Epoch 40/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.3028 - acc: 0.5170 - val_loss: 1.2519 - val_acc: 0.5646\n",
            "Epoch 41/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 1.2925 - acc: 0.5303 - val_loss: 1.2635 - val_acc: 0.5542\n",
            "Epoch 42/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.2883 - acc: 0.5294 - val_loss: 1.2502 - val_acc: 0.5609\n",
            "Epoch 43/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.2784 - acc: 0.5360 - val_loss: 1.2456 - val_acc: 0.5511\n",
            "Epoch 44/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.2679 - acc: 0.5454 - val_loss: 1.2179 - val_acc: 0.5695\n",
            "Epoch 45/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.2582 - acc: 0.5400 - val_loss: 1.2249 - val_acc: 0.5707\n",
            "Epoch 46/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.2597 - acc: 0.5321 - val_loss: 1.2376 - val_acc: 0.5456\n",
            "Epoch 47/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.2439 - acc: 0.5421 - val_loss: 1.2184 - val_acc: 0.5677\n",
            "Epoch 48/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 1.2347 - acc: 0.5451 - val_loss: 1.2161 - val_acc: 0.5597\n",
            "Epoch 49/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.2319 - acc: 0.5400 - val_loss: 1.1987 - val_acc: 0.5738\n",
            "Epoch 50/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.2285 - acc: 0.5493 - val_loss: 1.2066 - val_acc: 0.5677\n",
            "Epoch 51/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 1.2206 - acc: 0.5554 - val_loss: 1.1878 - val_acc: 0.5713\n",
            "Epoch 52/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 1.2061 - acc: 0.5608 - val_loss: 1.2017 - val_acc: 0.5713\n",
            "Epoch 53/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.2075 - acc: 0.5541 - val_loss: 1.1613 - val_acc: 0.5952\n",
            "Epoch 54/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.1850 - acc: 0.5747 - val_loss: 1.1609 - val_acc: 0.5897\n",
            "Epoch 55/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.1680 - acc: 0.5753 - val_loss: 1.1814 - val_acc: 0.5701\n",
            "Epoch 56/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.1774 - acc: 0.5722 - val_loss: 1.1616 - val_acc: 0.5860\n",
            "Epoch 57/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.1785 - acc: 0.5722 - val_loss: 1.1506 - val_acc: 0.5818\n",
            "Epoch 58/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.1597 - acc: 0.5813 - val_loss: 1.1807 - val_acc: 0.5713\n",
            "Epoch 59/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 1.1556 - acc: 0.5810 - val_loss: 1.1295 - val_acc: 0.6032\n",
            "Epoch 60/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 1.1577 - acc: 0.5786 - val_loss: 1.1596 - val_acc: 0.5640\n",
            "Epoch 61/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 1.1401 - acc: 0.5771 - val_loss: 1.1335 - val_acc: 0.5964\n",
            "Epoch 62/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 1.1352 - acc: 0.5913 - val_loss: 1.1203 - val_acc: 0.5971\n",
            "Epoch 63/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.1402 - acc: 0.5738 - val_loss: 1.1262 - val_acc: 0.5934\n",
            "Epoch 64/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.1353 - acc: 0.5976 - val_loss: 1.1081 - val_acc: 0.6069\n",
            "Epoch 65/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.1278 - acc: 0.5843 - val_loss: 1.1441 - val_acc: 0.5732\n",
            "Epoch 66/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 1.1193 - acc: 0.5916 - val_loss: 1.0959 - val_acc: 0.6197\n",
            "Epoch 67/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.1091 - acc: 0.5946 - val_loss: 1.1234 - val_acc: 0.5860\n",
            "Epoch 68/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 1.1120 - acc: 0.5913 - val_loss: 1.0964 - val_acc: 0.6050\n",
            "Epoch 69/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.1015 - acc: 0.5976 - val_loss: 1.0955 - val_acc: 0.6111\n",
            "Epoch 70/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.0935 - acc: 0.6078 - val_loss: 1.0814 - val_acc: 0.6179\n",
            "Epoch 71/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 1.0896 - acc: 0.6030 - val_loss: 1.0640 - val_acc: 0.6203\n",
            "Epoch 72/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.0820 - acc: 0.6015 - val_loss: 1.0898 - val_acc: 0.6032\n",
            "Epoch 73/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.0867 - acc: 0.5997 - val_loss: 1.0655 - val_acc: 0.6148\n",
            "Epoch 74/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.0606 - acc: 0.6184 - val_loss: 1.0628 - val_acc: 0.6209\n",
            "Epoch 75/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.0678 - acc: 0.6145 - val_loss: 1.0708 - val_acc: 0.6167\n",
            "Epoch 76/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.0747 - acc: 0.6127 - val_loss: 1.0790 - val_acc: 0.6118\n",
            "Epoch 77/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.0638 - acc: 0.6136 - val_loss: 1.0647 - val_acc: 0.6105\n",
            "Epoch 78/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.0525 - acc: 0.6133 - val_loss: 1.0402 - val_acc: 0.6252\n",
            "Epoch 79/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.0500 - acc: 0.6145 - val_loss: 1.0431 - val_acc: 0.6216\n",
            "Epoch 80/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.0461 - acc: 0.6133 - val_loss: 1.0611 - val_acc: 0.6142\n",
            "Epoch 81/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.0290 - acc: 0.6275 - val_loss: 1.0812 - val_acc: 0.5928\n",
            "Epoch 82/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.0383 - acc: 0.6244 - val_loss: 1.0333 - val_acc: 0.6356\n",
            "Epoch 83/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 1.0271 - acc: 0.6238 - val_loss: 1.0283 - val_acc: 0.6271\n",
            "Epoch 84/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 1.0306 - acc: 0.6214 - val_loss: 1.0360 - val_acc: 0.6344\n",
            "Epoch 85/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 1.0125 - acc: 0.6271 - val_loss: 1.0369 - val_acc: 0.6093\n",
            "Epoch 86/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 1.0281 - acc: 0.6199 - val_loss: 1.0019 - val_acc: 0.6430\n",
            "Epoch 87/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 1.0190 - acc: 0.6290 - val_loss: 1.0309 - val_acc: 0.6350\n",
            "Epoch 88/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 1.0052 - acc: 0.6326 - val_loss: 0.9930 - val_acc: 0.6448\n",
            "Epoch 89/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.9998 - acc: 0.6392 - val_loss: 1.0450 - val_acc: 0.6148\n",
            "Epoch 90/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.9857 - acc: 0.6434 - val_loss: 0.9878 - val_acc: 0.6485\n",
            "Epoch 91/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.9796 - acc: 0.6425 - val_loss: 0.9934 - val_acc: 0.6485\n",
            "Epoch 92/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.9862 - acc: 0.6501 - val_loss: 0.9991 - val_acc: 0.6448\n",
            "Epoch 93/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.9955 - acc: 0.6347 - val_loss: 0.9799 - val_acc: 0.6528\n",
            "Epoch 94/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.9958 - acc: 0.6383 - val_loss: 1.0040 - val_acc: 0.6399\n",
            "Epoch 95/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.9791 - acc: 0.6456 - val_loss: 0.9765 - val_acc: 0.6571\n",
            "Epoch 96/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.9572 - acc: 0.6501 - val_loss: 0.9981 - val_acc: 0.6387\n",
            "Epoch 97/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.9624 - acc: 0.6504 - val_loss: 0.9773 - val_acc: 0.6626\n",
            "Epoch 98/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.9556 - acc: 0.6471 - val_loss: 1.0070 - val_acc: 0.6222\n",
            "Epoch 99/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.9641 - acc: 0.6492 - val_loss: 0.9693 - val_acc: 0.6565\n",
            "Epoch 100/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.9454 - acc: 0.6516 - val_loss: 0.9618 - val_acc: 0.6558\n",
            "Epoch 101/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.9440 - acc: 0.6552 - val_loss: 0.9656 - val_acc: 0.6595\n",
            "Epoch 102/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.9347 - acc: 0.6576 - val_loss: 0.9794 - val_acc: 0.6448\n",
            "Epoch 103/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.9449 - acc: 0.6540 - val_loss: 0.9506 - val_acc: 0.6540\n",
            "Epoch 104/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.9513 - acc: 0.6516 - val_loss: 0.9545 - val_acc: 0.6540\n",
            "Epoch 105/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.9355 - acc: 0.6643 - val_loss: 0.9592 - val_acc: 0.6558\n",
            "Epoch 106/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.9288 - acc: 0.6682 - val_loss: 0.9249 - val_acc: 0.6712\n",
            "Epoch 107/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.9212 - acc: 0.6685 - val_loss: 0.9468 - val_acc: 0.6552\n",
            "Epoch 108/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.9302 - acc: 0.6591 - val_loss: 0.9376 - val_acc: 0.6669\n",
            "Epoch 109/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.9118 - acc: 0.6796 - val_loss: 0.9408 - val_acc: 0.6669\n",
            "Epoch 110/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.9030 - acc: 0.6796 - val_loss: 0.9219 - val_acc: 0.6693\n",
            "Epoch 111/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.9212 - acc: 0.6763 - val_loss: 0.9419 - val_acc: 0.6650\n",
            "Epoch 112/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.9105 - acc: 0.6691 - val_loss: 0.9655 - val_acc: 0.6479\n",
            "Epoch 113/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8979 - acc: 0.6808 - val_loss: 0.9092 - val_acc: 0.6736\n",
            "Epoch 114/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.9003 - acc: 0.6742 - val_loss: 0.9435 - val_acc: 0.6614\n",
            "Epoch 115/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8881 - acc: 0.6754 - val_loss: 0.9362 - val_acc: 0.6577\n",
            "Epoch 116/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8859 - acc: 0.6772 - val_loss: 0.9190 - val_acc: 0.6705\n",
            "Epoch 117/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8876 - acc: 0.6721 - val_loss: 0.8896 - val_acc: 0.6883\n",
            "Epoch 118/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.9003 - acc: 0.6703 - val_loss: 0.9236 - val_acc: 0.6595\n",
            "Epoch 119/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.8745 - acc: 0.6808 - val_loss: 0.8912 - val_acc: 0.6944\n",
            "Epoch 120/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8718 - acc: 0.6833 - val_loss: 0.9300 - val_acc: 0.6626\n",
            "Epoch 121/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.8736 - acc: 0.6863 - val_loss: 0.9234 - val_acc: 0.6601\n",
            "Epoch 122/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8815 - acc: 0.6805 - val_loss: 0.8950 - val_acc: 0.6846\n",
            "Epoch 123/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8588 - acc: 0.6920 - val_loss: 0.9433 - val_acc: 0.6516\n",
            "Epoch 124/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.8575 - acc: 0.6905 - val_loss: 0.9381 - val_acc: 0.6546\n",
            "Epoch 125/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.8612 - acc: 0.6938 - val_loss: 0.8732 - val_acc: 0.6828\n",
            "Epoch 126/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.8436 - acc: 0.6977 - val_loss: 0.8755 - val_acc: 0.6859\n",
            "Epoch 127/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.8415 - acc: 0.6986 - val_loss: 0.8857 - val_acc: 0.6797\n",
            "Epoch 128/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.8541 - acc: 0.6932 - val_loss: 0.9073 - val_acc: 0.6748\n",
            "Epoch 129/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8466 - acc: 0.6836 - val_loss: 0.8607 - val_acc: 0.6914\n",
            "Epoch 130/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.8443 - acc: 0.6989 - val_loss: 0.8826 - val_acc: 0.6736\n",
            "Epoch 131/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8469 - acc: 0.7026 - val_loss: 0.9031 - val_acc: 0.6589\n",
            "Epoch 132/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.8442 - acc: 0.6926 - val_loss: 0.8944 - val_acc: 0.6718\n",
            "Epoch 133/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.8307 - acc: 0.6923 - val_loss: 0.9029 - val_acc: 0.6638\n",
            "Epoch 134/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.8390 - acc: 0.6968 - val_loss: 0.8538 - val_acc: 0.6950\n",
            "Epoch 135/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.8272 - acc: 0.7065 - val_loss: 0.8714 - val_acc: 0.6883\n",
            "Epoch 136/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.8203 - acc: 0.7020 - val_loss: 0.8678 - val_acc: 0.6871\n",
            "Epoch 137/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.8213 - acc: 0.7062 - val_loss: 0.8519 - val_acc: 0.6944\n",
            "Epoch 138/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.8143 - acc: 0.7143 - val_loss: 0.8412 - val_acc: 0.6926\n",
            "Epoch 139/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8289 - acc: 0.7026 - val_loss: 0.8640 - val_acc: 0.6797\n",
            "Epoch 140/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.8165 - acc: 0.7041 - val_loss: 0.8431 - val_acc: 0.6852\n",
            "Epoch 141/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.8121 - acc: 0.7059 - val_loss: 0.8403 - val_acc: 0.6963\n",
            "Epoch 142/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.8081 - acc: 0.7044 - val_loss: 0.8384 - val_acc: 0.6963\n",
            "Epoch 143/1000\n",
            "3315/3315 [==============================] - 3s 949us/step - loss: 0.7890 - acc: 0.7149 - val_loss: 0.8350 - val_acc: 0.7048\n",
            "Epoch 144/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.8017 - acc: 0.6998 - val_loss: 0.8467 - val_acc: 0.6834\n",
            "Epoch 145/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.8095 - acc: 0.7107 - val_loss: 0.8382 - val_acc: 0.6852\n",
            "Epoch 146/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.7868 - acc: 0.7125 - val_loss: 0.8628 - val_acc: 0.6969\n",
            "Epoch 147/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.7862 - acc: 0.7234 - val_loss: 0.8174 - val_acc: 0.7085\n",
            "Epoch 148/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.7832 - acc: 0.7122 - val_loss: 0.8378 - val_acc: 0.6950\n",
            "Epoch 149/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.7858 - acc: 0.7164 - val_loss: 0.8187 - val_acc: 0.6987\n",
            "Epoch 150/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.7758 - acc: 0.7149 - val_loss: 0.8299 - val_acc: 0.7048\n",
            "Epoch 151/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.7796 - acc: 0.7201 - val_loss: 0.8110 - val_acc: 0.7183\n",
            "Epoch 152/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7706 - acc: 0.7216 - val_loss: 0.8374 - val_acc: 0.6822\n",
            "Epoch 153/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.7757 - acc: 0.7186 - val_loss: 0.8432 - val_acc: 0.6748\n",
            "Epoch 154/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.7802 - acc: 0.7176 - val_loss: 0.8274 - val_acc: 0.7024\n",
            "Epoch 155/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.7599 - acc: 0.7288 - val_loss: 0.8234 - val_acc: 0.7061\n",
            "Epoch 156/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.7567 - acc: 0.7351 - val_loss: 0.8251 - val_acc: 0.6920\n",
            "Epoch 157/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.7559 - acc: 0.7348 - val_loss: 0.7928 - val_acc: 0.7122\n",
            "Epoch 158/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.7472 - acc: 0.7318 - val_loss: 0.8014 - val_acc: 0.7250\n",
            "Epoch 159/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.7641 - acc: 0.7167 - val_loss: 0.7978 - val_acc: 0.6999\n",
            "Epoch 160/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.7585 - acc: 0.7261 - val_loss: 0.8171 - val_acc: 0.7030\n",
            "Epoch 161/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.7520 - acc: 0.7279 - val_loss: 0.8063 - val_acc: 0.7214\n",
            "Epoch 162/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.7432 - acc: 0.7324 - val_loss: 0.7965 - val_acc: 0.7042\n",
            "Epoch 163/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.7388 - acc: 0.7324 - val_loss: 0.8322 - val_acc: 0.6920\n",
            "Epoch 164/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7466 - acc: 0.7285 - val_loss: 0.8156 - val_acc: 0.6957\n",
            "Epoch 165/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.7336 - acc: 0.7421 - val_loss: 0.7903 - val_acc: 0.7103\n",
            "Epoch 166/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7324 - acc: 0.7306 - val_loss: 0.7838 - val_acc: 0.7122\n",
            "Epoch 167/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.7253 - acc: 0.7373 - val_loss: 0.7936 - val_acc: 0.7067\n",
            "Epoch 168/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7305 - acc: 0.7385 - val_loss: 0.7832 - val_acc: 0.7257\n",
            "Epoch 169/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.7367 - acc: 0.7354 - val_loss: 0.8131 - val_acc: 0.6975\n",
            "Epoch 170/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.7149 - acc: 0.7370 - val_loss: 0.7811 - val_acc: 0.7036\n",
            "Epoch 171/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.7157 - acc: 0.7487 - val_loss: 0.7664 - val_acc: 0.7232\n",
            "Epoch 172/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.7207 - acc: 0.7367 - val_loss: 0.7898 - val_acc: 0.7079\n",
            "Epoch 173/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7179 - acc: 0.7463 - val_loss: 0.7920 - val_acc: 0.7079\n",
            "Epoch 174/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.7108 - acc: 0.7385 - val_loss: 0.7737 - val_acc: 0.7067\n",
            "Epoch 175/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.7160 - acc: 0.7412 - val_loss: 0.7665 - val_acc: 0.7226\n",
            "Epoch 176/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.7084 - acc: 0.7409 - val_loss: 0.7575 - val_acc: 0.7330\n",
            "Epoch 177/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.7078 - acc: 0.7475 - val_loss: 0.7654 - val_acc: 0.7208\n",
            "Epoch 178/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.6877 - acc: 0.7551 - val_loss: 0.7851 - val_acc: 0.7085\n",
            "Epoch 179/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.6923 - acc: 0.7472 - val_loss: 0.7622 - val_acc: 0.7263\n",
            "Epoch 180/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.7008 - acc: 0.7490 - val_loss: 0.7435 - val_acc: 0.7385\n",
            "Epoch 181/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.6917 - acc: 0.7538 - val_loss: 0.7409 - val_acc: 0.7306\n",
            "Epoch 182/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.7012 - acc: 0.7445 - val_loss: 0.7989 - val_acc: 0.6981\n",
            "Epoch 183/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.7064 - acc: 0.7394 - val_loss: 0.7386 - val_acc: 0.7361\n",
            "Epoch 184/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.6987 - acc: 0.7496 - val_loss: 0.7533 - val_acc: 0.7287\n",
            "Epoch 185/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.6872 - acc: 0.7544 - val_loss: 0.7829 - val_acc: 0.7110\n",
            "Epoch 186/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.6733 - acc: 0.7544 - val_loss: 0.7405 - val_acc: 0.7306\n",
            "Epoch 187/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.6872 - acc: 0.7544 - val_loss: 0.7494 - val_acc: 0.7299\n",
            "Epoch 188/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.6801 - acc: 0.7572 - val_loss: 0.7360 - val_acc: 0.7287\n",
            "Epoch 189/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.6730 - acc: 0.7523 - val_loss: 0.7357 - val_acc: 0.7355\n",
            "Epoch 190/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.6822 - acc: 0.7581 - val_loss: 0.7538 - val_acc: 0.7263\n",
            "Epoch 191/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.6789 - acc: 0.7560 - val_loss: 0.7464 - val_acc: 0.7134\n",
            "Epoch 192/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.6664 - acc: 0.7581 - val_loss: 0.7254 - val_acc: 0.7373\n",
            "Epoch 193/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.6625 - acc: 0.7638 - val_loss: 0.7337 - val_acc: 0.7293\n",
            "Epoch 194/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.6533 - acc: 0.7626 - val_loss: 0.7508 - val_acc: 0.7250\n",
            "Epoch 195/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.6612 - acc: 0.7608 - val_loss: 0.7182 - val_acc: 0.7428\n",
            "Epoch 196/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.6632 - acc: 0.7626 - val_loss: 0.7676 - val_acc: 0.7103\n",
            "Epoch 197/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.6519 - acc: 0.7716 - val_loss: 0.7367 - val_acc: 0.7281\n",
            "Epoch 198/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.6786 - acc: 0.7457 - val_loss: 0.7312 - val_acc: 0.7238\n",
            "Epoch 199/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.6513 - acc: 0.7650 - val_loss: 0.7681 - val_acc: 0.7091\n",
            "Epoch 200/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.6691 - acc: 0.7626 - val_loss: 0.7228 - val_acc: 0.7324\n",
            "Epoch 201/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.6479 - acc: 0.7689 - val_loss: 0.7155 - val_acc: 0.7428\n",
            "Epoch 202/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.6407 - acc: 0.7656 - val_loss: 0.7281 - val_acc: 0.7336\n",
            "Epoch 203/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.6420 - acc: 0.7707 - val_loss: 0.7163 - val_acc: 0.7422\n",
            "Epoch 204/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.6468 - acc: 0.7671 - val_loss: 0.7396 - val_acc: 0.7299\n",
            "Epoch 205/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.6410 - acc: 0.7653 - val_loss: 0.7029 - val_acc: 0.7502\n",
            "Epoch 206/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.6382 - acc: 0.7725 - val_loss: 0.7147 - val_acc: 0.7514\n",
            "Epoch 207/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.6279 - acc: 0.7810 - val_loss: 0.7148 - val_acc: 0.7373\n",
            "Epoch 208/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.6330 - acc: 0.7795 - val_loss: 0.7362 - val_acc: 0.7189\n",
            "Epoch 209/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.6365 - acc: 0.7710 - val_loss: 0.7026 - val_acc: 0.7453\n",
            "Epoch 210/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.6235 - acc: 0.7771 - val_loss: 0.6977 - val_acc: 0.7489\n",
            "Epoch 211/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.6201 - acc: 0.7771 - val_loss: 0.6955 - val_acc: 0.7575\n",
            "Epoch 212/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.6273 - acc: 0.7768 - val_loss: 0.7051 - val_acc: 0.7508\n",
            "Epoch 213/1000\n",
            "3315/3315 [==============================] - 1s 403us/step - loss: 0.6208 - acc: 0.7765 - val_loss: 0.7158 - val_acc: 0.7306\n",
            "Epoch 214/1000\n",
            "3315/3315 [==============================] - 1s 403us/step - loss: 0.6104 - acc: 0.7816 - val_loss: 0.6691 - val_acc: 0.7630\n",
            "Epoch 215/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.6261 - acc: 0.7716 - val_loss: 0.6718 - val_acc: 0.7581\n",
            "Epoch 216/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.6074 - acc: 0.7762 - val_loss: 0.7026 - val_acc: 0.7410\n",
            "Epoch 217/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.6193 - acc: 0.7798 - val_loss: 0.7059 - val_acc: 0.7440\n",
            "Epoch 218/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.6078 - acc: 0.7825 - val_loss: 0.6756 - val_acc: 0.7520\n",
            "Epoch 219/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.6167 - acc: 0.7777 - val_loss: 0.6981 - val_acc: 0.7348\n",
            "Epoch 220/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.6005 - acc: 0.7810 - val_loss: 0.7103 - val_acc: 0.7367\n",
            "Epoch 221/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.6070 - acc: 0.7876 - val_loss: 0.6851 - val_acc: 0.7477\n",
            "Epoch 222/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.6108 - acc: 0.7777 - val_loss: 0.6708 - val_acc: 0.7606\n",
            "Epoch 223/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.5894 - acc: 0.7876 - val_loss: 0.6840 - val_acc: 0.7495\n",
            "Epoch 224/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.6110 - acc: 0.7804 - val_loss: 0.6666 - val_acc: 0.7593\n",
            "Epoch 225/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.5883 - acc: 0.7888 - val_loss: 0.7091 - val_acc: 0.7385\n",
            "Epoch 226/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.5979 - acc: 0.7861 - val_loss: 0.6805 - val_acc: 0.7636\n",
            "Epoch 227/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5957 - acc: 0.7885 - val_loss: 0.6606 - val_acc: 0.7618\n",
            "Epoch 228/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.5813 - acc: 0.7970 - val_loss: 0.6695 - val_acc: 0.7648\n",
            "Epoch 229/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.5906 - acc: 0.7873 - val_loss: 0.6844 - val_acc: 0.7514\n",
            "Epoch 230/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.5875 - acc: 0.7946 - val_loss: 0.6766 - val_acc: 0.7569\n",
            "Epoch 231/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.5943 - acc: 0.7870 - val_loss: 0.6659 - val_acc: 0.7685\n",
            "Epoch 232/1000\n",
            "3315/3315 [==============================] - 1s 408us/step - loss: 0.5974 - acc: 0.7913 - val_loss: 0.6720 - val_acc: 0.7593\n",
            "Epoch 233/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.5822 - acc: 0.7967 - val_loss: 0.6576 - val_acc: 0.7685\n",
            "Epoch 234/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5796 - acc: 0.7955 - val_loss: 0.7083 - val_acc: 0.7544\n",
            "Epoch 235/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.5680 - acc: 0.8024 - val_loss: 0.6499 - val_acc: 0.7673\n",
            "Epoch 236/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.5857 - acc: 0.7943 - val_loss: 0.6745 - val_acc: 0.7495\n",
            "Epoch 237/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.5582 - acc: 0.8024 - val_loss: 0.6996 - val_acc: 0.7361\n",
            "Epoch 238/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.5696 - acc: 0.8006 - val_loss: 0.6591 - val_acc: 0.7624\n",
            "Epoch 239/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.5745 - acc: 0.7967 - val_loss: 0.6432 - val_acc: 0.7679\n",
            "Epoch 240/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.5819 - acc: 0.7900 - val_loss: 0.6521 - val_acc: 0.7691\n",
            "Epoch 241/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.5833 - acc: 0.7888 - val_loss: 0.6670 - val_acc: 0.7520\n",
            "Epoch 242/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.5554 - acc: 0.8003 - val_loss: 0.6823 - val_acc: 0.7520\n",
            "Epoch 243/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.5650 - acc: 0.8000 - val_loss: 0.6498 - val_acc: 0.7667\n",
            "Epoch 244/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.5709 - acc: 0.8006 - val_loss: 0.6426 - val_acc: 0.7661\n",
            "Epoch 245/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5558 - acc: 0.8051 - val_loss: 0.6409 - val_acc: 0.7691\n",
            "Epoch 246/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5595 - acc: 0.8045 - val_loss: 0.6690 - val_acc: 0.7428\n",
            "Epoch 247/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5543 - acc: 0.8094 - val_loss: 0.6763 - val_acc: 0.7526\n",
            "Epoch 248/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5539 - acc: 0.8030 - val_loss: 0.6382 - val_acc: 0.7722\n",
            "Epoch 249/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5493 - acc: 0.8124 - val_loss: 0.6129 - val_acc: 0.7851\n",
            "Epoch 250/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.5646 - acc: 0.7988 - val_loss: 0.6363 - val_acc: 0.7722\n",
            "Epoch 251/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.5546 - acc: 0.8036 - val_loss: 0.6284 - val_acc: 0.7863\n",
            "Epoch 252/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.5510 - acc: 0.8009 - val_loss: 0.6307 - val_acc: 0.7759\n",
            "Epoch 253/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.5368 - acc: 0.8151 - val_loss: 0.6207 - val_acc: 0.7753\n",
            "Epoch 254/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.5475 - acc: 0.8018 - val_loss: 0.6369 - val_acc: 0.7685\n",
            "Epoch 255/1000\n",
            "3315/3315 [==============================] - 1s 403us/step - loss: 0.5361 - acc: 0.8148 - val_loss: 0.6222 - val_acc: 0.7783\n",
            "Epoch 256/1000\n",
            "3315/3315 [==============================] - 1s 400us/step - loss: 0.5342 - acc: 0.8097 - val_loss: 0.6130 - val_acc: 0.7765\n",
            "Epoch 257/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.5398 - acc: 0.8027 - val_loss: 0.6302 - val_acc: 0.7802\n",
            "Epoch 258/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.5317 - acc: 0.8072 - val_loss: 0.6192 - val_acc: 0.7838\n",
            "Epoch 259/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.5274 - acc: 0.8145 - val_loss: 0.6269 - val_acc: 0.7673\n",
            "Epoch 260/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.5447 - acc: 0.8081 - val_loss: 0.6417 - val_acc: 0.7685\n",
            "Epoch 261/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.5400 - acc: 0.8048 - val_loss: 0.6331 - val_acc: 0.7704\n",
            "Epoch 262/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.5437 - acc: 0.8039 - val_loss: 0.6217 - val_acc: 0.7728\n",
            "Epoch 263/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.5363 - acc: 0.8133 - val_loss: 0.6172 - val_acc: 0.7826\n",
            "Epoch 264/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.5208 - acc: 0.8136 - val_loss: 0.6724 - val_acc: 0.7477\n",
            "Epoch 265/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.5173 - acc: 0.8205 - val_loss: 0.6371 - val_acc: 0.7759\n",
            "Epoch 266/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5288 - acc: 0.8087 - val_loss: 0.6084 - val_acc: 0.7875\n",
            "Epoch 267/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5307 - acc: 0.8127 - val_loss: 0.6155 - val_acc: 0.7844\n",
            "Epoch 268/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5205 - acc: 0.8193 - val_loss: 0.6140 - val_acc: 0.7789\n",
            "Epoch 269/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.5263 - acc: 0.8103 - val_loss: 0.5982 - val_acc: 0.8016\n",
            "Epoch 270/1000\n",
            "3315/3315 [==============================] - 1s 402us/step - loss: 0.5257 - acc: 0.8136 - val_loss: 0.6234 - val_acc: 0.7661\n",
            "Epoch 271/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.5186 - acc: 0.8181 - val_loss: 0.6171 - val_acc: 0.7759\n",
            "Epoch 272/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.5190 - acc: 0.8106 - val_loss: 0.6116 - val_acc: 0.7802\n",
            "Epoch 273/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5136 - acc: 0.8139 - val_loss: 0.5871 - val_acc: 0.7869\n",
            "Epoch 274/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.5194 - acc: 0.8139 - val_loss: 0.6287 - val_acc: 0.7704\n",
            "Epoch 275/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.5186 - acc: 0.8166 - val_loss: 0.6051 - val_acc: 0.7906\n",
            "Epoch 276/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.5150 - acc: 0.8181 - val_loss: 0.5986 - val_acc: 0.7863\n",
            "Epoch 277/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.4948 - acc: 0.8244 - val_loss: 0.6257 - val_acc: 0.7704\n",
            "Epoch 278/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.5006 - acc: 0.8247 - val_loss: 0.5813 - val_acc: 0.7955\n",
            "Epoch 279/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.5047 - acc: 0.8199 - val_loss: 0.6408 - val_acc: 0.7648\n",
            "Epoch 280/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.5122 - acc: 0.8166 - val_loss: 0.5947 - val_acc: 0.7881\n",
            "Epoch 281/1000\n",
            "3315/3315 [==============================] - 1s 401us/step - loss: 0.5068 - acc: 0.8184 - val_loss: 0.6103 - val_acc: 0.7832\n",
            "Epoch 282/1000\n",
            "3315/3315 [==============================] - 1s 400us/step - loss: 0.4971 - acc: 0.8214 - val_loss: 0.6313 - val_acc: 0.7575\n",
            "Epoch 283/1000\n",
            "3315/3315 [==============================] - 1s 408us/step - loss: 0.4949 - acc: 0.8275 - val_loss: 0.5841 - val_acc: 0.7906\n",
            "Epoch 284/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.4949 - acc: 0.8281 - val_loss: 0.5846 - val_acc: 0.7893\n",
            "Epoch 285/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.4971 - acc: 0.8244 - val_loss: 0.5624 - val_acc: 0.8034\n",
            "Epoch 286/1000\n",
            "3315/3315 [==============================] - 1s 397us/step - loss: 0.4816 - acc: 0.8302 - val_loss: 0.5761 - val_acc: 0.7998\n",
            "Epoch 287/1000\n",
            "3315/3315 [==============================] - 1s 392us/step - loss: 0.4916 - acc: 0.8320 - val_loss: 0.5786 - val_acc: 0.7924\n",
            "Epoch 288/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.4769 - acc: 0.8320 - val_loss: 0.5809 - val_acc: 0.7961\n",
            "Epoch 289/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.4841 - acc: 0.8329 - val_loss: 0.5831 - val_acc: 0.7869\n",
            "Epoch 290/1000\n",
            "3315/3315 [==============================] - 1s 398us/step - loss: 0.4788 - acc: 0.8281 - val_loss: 0.5640 - val_acc: 0.7906\n",
            "Epoch 291/1000\n",
            "3315/3315 [==============================] - 1s 397us/step - loss: 0.4779 - acc: 0.8253 - val_loss: 0.5767 - val_acc: 0.7985\n",
            "Epoch 292/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.4895 - acc: 0.8302 - val_loss: 0.5918 - val_acc: 0.7838\n",
            "Epoch 293/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.4834 - acc: 0.8311 - val_loss: 0.5890 - val_acc: 0.7900\n",
            "Epoch 294/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.4879 - acc: 0.8326 - val_loss: 0.5778 - val_acc: 0.7942\n",
            "Epoch 295/1000\n",
            "3315/3315 [==============================] - 1s 402us/step - loss: 0.4858 - acc: 0.8256 - val_loss: 0.6282 - val_acc: 0.7697\n",
            "Epoch 296/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.4679 - acc: 0.8419 - val_loss: 0.5778 - val_acc: 0.7961\n",
            "Epoch 297/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.4708 - acc: 0.8377 - val_loss: 0.5804 - val_acc: 0.7918\n",
            "Epoch 298/1000\n",
            "3315/3315 [==============================] - 1s 398us/step - loss: 0.4691 - acc: 0.8338 - val_loss: 0.5646 - val_acc: 0.8034\n",
            "Epoch 299/1000\n",
            "3315/3315 [==============================] - 1s 391us/step - loss: 0.4816 - acc: 0.8350 - val_loss: 0.5958 - val_acc: 0.7881\n",
            "Epoch 300/1000\n",
            "3315/3315 [==============================] - 1s 398us/step - loss: 0.4793 - acc: 0.8308 - val_loss: 0.5686 - val_acc: 0.8022\n",
            "Epoch 301/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.4655 - acc: 0.8308 - val_loss: 0.6170 - val_acc: 0.7716\n",
            "Epoch 302/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.4734 - acc: 0.8413 - val_loss: 0.5565 - val_acc: 0.7991\n",
            "Epoch 303/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.4756 - acc: 0.8398 - val_loss: 0.5610 - val_acc: 0.7949\n",
            "Epoch 304/1000\n",
            "3315/3315 [==============================] - 1s 394us/step - loss: 0.4684 - acc: 0.8332 - val_loss: 0.5564 - val_acc: 0.8120\n",
            "Epoch 305/1000\n",
            "3315/3315 [==============================] - 1s 396us/step - loss: 0.4642 - acc: 0.8347 - val_loss: 0.5855 - val_acc: 0.7949\n",
            "Epoch 306/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.4586 - acc: 0.8452 - val_loss: 0.5689 - val_acc: 0.7942\n",
            "Epoch 307/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.4568 - acc: 0.8383 - val_loss: 0.5932 - val_acc: 0.7740\n",
            "Epoch 308/1000\n",
            "3315/3315 [==============================] - 1s 400us/step - loss: 0.4473 - acc: 0.8495 - val_loss: 0.5662 - val_acc: 0.7949\n",
            "Epoch 309/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.4509 - acc: 0.8422 - val_loss: 0.5625 - val_acc: 0.7967\n",
            "Epoch 310/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.4492 - acc: 0.8398 - val_loss: 0.5682 - val_acc: 0.7900\n",
            "Epoch 311/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.4526 - acc: 0.8380 - val_loss: 0.5478 - val_acc: 0.8016\n",
            "Epoch 312/1000\n",
            "3315/3315 [==============================] - 1s 408us/step - loss: 0.4592 - acc: 0.8401 - val_loss: 0.5608 - val_acc: 0.7998\n",
            "Epoch 313/1000\n",
            "3315/3315 [==============================] - 1s 401us/step - loss: 0.4400 - acc: 0.8504 - val_loss: 0.5470 - val_acc: 0.8126\n",
            "Epoch 314/1000\n",
            "3315/3315 [==============================] - 1s 408us/step - loss: 0.4440 - acc: 0.8465 - val_loss: 0.5604 - val_acc: 0.8004\n",
            "Epoch 315/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.4481 - acc: 0.8477 - val_loss: 0.5704 - val_acc: 0.7985\n",
            "Epoch 316/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.4395 - acc: 0.8531 - val_loss: 0.5684 - val_acc: 0.8010\n",
            "Epoch 317/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.4442 - acc: 0.8422 - val_loss: 0.5595 - val_acc: 0.7991\n",
            "Epoch 318/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.4451 - acc: 0.8483 - val_loss: 0.5404 - val_acc: 0.8157\n",
            "Epoch 319/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.4465 - acc: 0.8459 - val_loss: 0.5528 - val_acc: 0.8083\n",
            "Epoch 320/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.4522 - acc: 0.8443 - val_loss: 0.5475 - val_acc: 0.8083\n",
            "Epoch 321/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.4458 - acc: 0.8428 - val_loss: 0.5713 - val_acc: 0.7955\n",
            "Epoch 322/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.4331 - acc: 0.8543 - val_loss: 0.5397 - val_acc: 0.8102\n",
            "Epoch 323/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.4437 - acc: 0.8404 - val_loss: 0.5409 - val_acc: 0.8096\n",
            "Epoch 324/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.4393 - acc: 0.8498 - val_loss: 0.5285 - val_acc: 0.8249\n",
            "Epoch 325/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.4503 - acc: 0.8416 - val_loss: 0.5929 - val_acc: 0.7857\n",
            "Epoch 326/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.4306 - acc: 0.8474 - val_loss: 0.5381 - val_acc: 0.8145\n",
            "Epoch 327/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.4406 - acc: 0.8492 - val_loss: 0.5219 - val_acc: 0.8169\n",
            "Epoch 328/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.4297 - acc: 0.8480 - val_loss: 0.5301 - val_acc: 0.8145\n",
            "Epoch 329/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.4267 - acc: 0.8519 - val_loss: 0.5394 - val_acc: 0.8065\n",
            "Epoch 330/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.4371 - acc: 0.8513 - val_loss: 0.5384 - val_acc: 0.8114\n",
            "Epoch 331/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.4353 - acc: 0.8525 - val_loss: 0.5244 - val_acc: 0.8175\n",
            "Epoch 332/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.4371 - acc: 0.8446 - val_loss: 0.5499 - val_acc: 0.8022\n",
            "Epoch 333/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.4222 - acc: 0.8534 - val_loss: 0.5238 - val_acc: 0.8181\n",
            "Epoch 334/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.4332 - acc: 0.8504 - val_loss: 0.5576 - val_acc: 0.7967\n",
            "Epoch 335/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.4191 - acc: 0.8594 - val_loss: 0.5491 - val_acc: 0.8047\n",
            "Epoch 336/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.4316 - acc: 0.8543 - val_loss: 0.5302 - val_acc: 0.8212\n",
            "Epoch 337/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.4340 - acc: 0.8477 - val_loss: 0.5091 - val_acc: 0.8206\n",
            "Epoch 338/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.4232 - acc: 0.8540 - val_loss: 0.5244 - val_acc: 0.8163\n",
            "Epoch 339/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.4203 - acc: 0.8501 - val_loss: 0.5284 - val_acc: 0.8151\n",
            "Epoch 340/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.4183 - acc: 0.8579 - val_loss: 0.5259 - val_acc: 0.8120\n",
            "Epoch 341/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.4214 - acc: 0.8501 - val_loss: 0.5050 - val_acc: 0.8236\n",
            "Epoch 342/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.4186 - acc: 0.8537 - val_loss: 0.5273 - val_acc: 0.8200\n",
            "Epoch 343/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.4219 - acc: 0.8594 - val_loss: 0.5103 - val_acc: 0.8224\n",
            "Epoch 344/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.4156 - acc: 0.8594 - val_loss: 0.5296 - val_acc: 0.8145\n",
            "Epoch 345/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.4085 - acc: 0.8600 - val_loss: 0.4980 - val_acc: 0.8285\n",
            "Epoch 346/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.4126 - acc: 0.8567 - val_loss: 0.5245 - val_acc: 0.8261\n",
            "Epoch 347/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.4087 - acc: 0.8549 - val_loss: 0.5122 - val_acc: 0.8200\n",
            "Epoch 348/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3925 - acc: 0.8597 - val_loss: 0.5231 - val_acc: 0.8175\n",
            "Epoch 349/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.4132 - acc: 0.8618 - val_loss: 0.5188 - val_acc: 0.8120\n",
            "Epoch 350/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.4144 - acc: 0.8543 - val_loss: 0.5147 - val_acc: 0.8187\n",
            "Epoch 351/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.3994 - acc: 0.8606 - val_loss: 0.4923 - val_acc: 0.8383\n",
            "Epoch 352/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.4156 - acc: 0.8519 - val_loss: 0.5383 - val_acc: 0.8102\n",
            "Epoch 353/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.3942 - acc: 0.8633 - val_loss: 0.4955 - val_acc: 0.8261\n",
            "Epoch 354/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.4050 - acc: 0.8549 - val_loss: 0.5178 - val_acc: 0.8230\n",
            "Epoch 355/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.4104 - acc: 0.8579 - val_loss: 0.5204 - val_acc: 0.8096\n",
            "Epoch 356/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3901 - acc: 0.8630 - val_loss: 0.5064 - val_acc: 0.8230\n",
            "Epoch 357/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.3891 - acc: 0.8627 - val_loss: 0.5055 - val_acc: 0.8181\n",
            "Epoch 358/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.4066 - acc: 0.8573 - val_loss: 0.5406 - val_acc: 0.8163\n",
            "Epoch 359/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.4112 - acc: 0.8609 - val_loss: 0.4796 - val_acc: 0.8414\n",
            "Epoch 360/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3892 - acc: 0.8694 - val_loss: 0.5060 - val_acc: 0.8157\n",
            "Epoch 361/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.3982 - acc: 0.8570 - val_loss: 0.5120 - val_acc: 0.8120\n",
            "Epoch 362/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3904 - acc: 0.8621 - val_loss: 0.4950 - val_acc: 0.8347\n",
            "Epoch 363/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3953 - acc: 0.8588 - val_loss: 0.5007 - val_acc: 0.8249\n",
            "Epoch 364/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.4129 - acc: 0.8531 - val_loss: 0.5067 - val_acc: 0.8230\n",
            "Epoch 365/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.3951 - acc: 0.8633 - val_loss: 0.5013 - val_acc: 0.8322\n",
            "Epoch 366/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3888 - acc: 0.8612 - val_loss: 0.5299 - val_acc: 0.8169\n",
            "Epoch 367/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.3837 - acc: 0.8609 - val_loss: 0.4938 - val_acc: 0.8377\n",
            "Epoch 368/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.3818 - acc: 0.8673 - val_loss: 0.4943 - val_acc: 0.8224\n",
            "Epoch 369/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.3878 - acc: 0.8697 - val_loss: 0.4992 - val_acc: 0.8310\n",
            "Epoch 370/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3872 - acc: 0.8652 - val_loss: 0.4816 - val_acc: 0.8255\n",
            "Epoch 371/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.3801 - acc: 0.8655 - val_loss: 0.4672 - val_acc: 0.8438\n",
            "Epoch 372/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3797 - acc: 0.8661 - val_loss: 0.4828 - val_acc: 0.8402\n",
            "Epoch 373/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.3808 - acc: 0.8582 - val_loss: 0.4872 - val_acc: 0.8353\n",
            "Epoch 374/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.3832 - acc: 0.8679 - val_loss: 0.4922 - val_acc: 0.8242\n",
            "Epoch 375/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3656 - acc: 0.8808 - val_loss: 0.5173 - val_acc: 0.8132\n",
            "Epoch 376/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.3773 - acc: 0.8796 - val_loss: 0.5005 - val_acc: 0.8291\n",
            "Epoch 377/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.3803 - acc: 0.8685 - val_loss: 0.5132 - val_acc: 0.8157\n",
            "Epoch 378/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3678 - acc: 0.8808 - val_loss: 0.4851 - val_acc: 0.8322\n",
            "Epoch 379/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3804 - acc: 0.8685 - val_loss: 0.4848 - val_acc: 0.8298\n",
            "Epoch 380/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3738 - acc: 0.8739 - val_loss: 0.4921 - val_acc: 0.8347\n",
            "Epoch 381/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3681 - acc: 0.8727 - val_loss: 0.5270 - val_acc: 0.8065\n",
            "Epoch 382/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3779 - acc: 0.8730 - val_loss: 0.4757 - val_acc: 0.8310\n",
            "Epoch 383/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3704 - acc: 0.8739 - val_loss: 0.4949 - val_acc: 0.8291\n",
            "Epoch 384/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.3575 - acc: 0.8787 - val_loss: 0.5018 - val_acc: 0.8218\n",
            "Epoch 385/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.3849 - acc: 0.8670 - val_loss: 0.4957 - val_acc: 0.8298\n",
            "Epoch 386/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.3674 - acc: 0.8712 - val_loss: 0.4941 - val_acc: 0.8298\n",
            "Epoch 387/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.3664 - acc: 0.8751 - val_loss: 0.4796 - val_acc: 0.8396\n",
            "Epoch 388/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3584 - acc: 0.8775 - val_loss: 0.4768 - val_acc: 0.8371\n",
            "Epoch 389/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3805 - acc: 0.8712 - val_loss: 0.5013 - val_acc: 0.8212\n",
            "Epoch 390/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.3675 - acc: 0.8691 - val_loss: 0.5032 - val_acc: 0.8157\n",
            "Epoch 391/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.3636 - acc: 0.8772 - val_loss: 0.4799 - val_acc: 0.8310\n",
            "Epoch 392/1000\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.3566 - acc: 0.8778 - val_loss: 0.4635 - val_acc: 0.8402\n",
            "Epoch 393/1000\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.3657 - acc: 0.8703 - val_loss: 0.4837 - val_acc: 0.8340\n",
            "Epoch 394/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.3521 - acc: 0.8769 - val_loss: 0.4657 - val_acc: 0.8383\n",
            "Epoch 395/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.3653 - acc: 0.8694 - val_loss: 0.4610 - val_acc: 0.8451\n",
            "Epoch 396/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3687 - acc: 0.8715 - val_loss: 0.4671 - val_acc: 0.8322\n",
            "Epoch 397/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3580 - acc: 0.8766 - val_loss: 0.4657 - val_acc: 0.8420\n",
            "Epoch 398/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3721 - acc: 0.8670 - val_loss: 0.5146 - val_acc: 0.8163\n",
            "Epoch 399/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.3606 - acc: 0.8763 - val_loss: 0.4832 - val_acc: 0.8310\n",
            "Epoch 400/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3642 - acc: 0.8736 - val_loss: 0.4575 - val_acc: 0.8512\n",
            "Epoch 401/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3494 - acc: 0.8781 - val_loss: 0.4709 - val_acc: 0.8402\n",
            "Epoch 402/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.3534 - acc: 0.8830 - val_loss: 0.4918 - val_acc: 0.8328\n",
            "Epoch 403/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3461 - acc: 0.8796 - val_loss: 0.4605 - val_acc: 0.8414\n",
            "Epoch 404/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3467 - acc: 0.8790 - val_loss: 0.4654 - val_acc: 0.8396\n",
            "Epoch 405/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3603 - acc: 0.8709 - val_loss: 0.4941 - val_acc: 0.8242\n",
            "Epoch 406/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3484 - acc: 0.8799 - val_loss: 0.4639 - val_acc: 0.8438\n",
            "Epoch 407/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3409 - acc: 0.8827 - val_loss: 0.4501 - val_acc: 0.8512\n",
            "Epoch 408/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.3560 - acc: 0.8793 - val_loss: 0.4842 - val_acc: 0.8347\n",
            "Epoch 409/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.3342 - acc: 0.8908 - val_loss: 0.4742 - val_acc: 0.8377\n",
            "Epoch 410/1000\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.3361 - acc: 0.8860 - val_loss: 0.4836 - val_acc: 0.8310\n",
            "Epoch 411/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.3351 - acc: 0.8881 - val_loss: 0.4653 - val_acc: 0.8353\n",
            "Epoch 412/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3505 - acc: 0.8811 - val_loss: 0.4749 - val_acc: 0.8383\n",
            "Epoch 413/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3452 - acc: 0.8827 - val_loss: 0.4476 - val_acc: 0.8506\n",
            "Epoch 414/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3331 - acc: 0.8860 - val_loss: 0.4824 - val_acc: 0.8218\n",
            "Epoch 415/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3351 - acc: 0.8830 - val_loss: 0.4520 - val_acc: 0.8438\n",
            "Epoch 416/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3416 - acc: 0.8790 - val_loss: 0.4740 - val_acc: 0.8371\n",
            "Epoch 417/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3385 - acc: 0.8842 - val_loss: 0.4482 - val_acc: 0.8377\n",
            "Epoch 418/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.3311 - acc: 0.8805 - val_loss: 0.4560 - val_acc: 0.8506\n",
            "Epoch 419/1000\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.3383 - acc: 0.8845 - val_loss: 0.4632 - val_acc: 0.8420\n",
            "Epoch 420/1000\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.3286 - acc: 0.8878 - val_loss: 0.4668 - val_acc: 0.8475\n",
            "Epoch 421/1000\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.3473 - acc: 0.8808 - val_loss: 0.4616 - val_acc: 0.8463\n",
            "Epoch 422/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.3330 - acc: 0.8857 - val_loss: 0.4553 - val_acc: 0.8475\n",
            "Epoch 423/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3350 - acc: 0.8842 - val_loss: 0.4446 - val_acc: 0.8481\n",
            "Epoch 424/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3324 - acc: 0.8836 - val_loss: 0.4515 - val_acc: 0.8500\n",
            "Epoch 425/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.3338 - acc: 0.8899 - val_loss: 0.4694 - val_acc: 0.8377\n",
            "Epoch 426/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.3297 - acc: 0.8863 - val_loss: 0.4569 - val_acc: 0.8420\n",
            "Epoch 427/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3401 - acc: 0.8833 - val_loss: 0.4306 - val_acc: 0.8561\n",
            "Epoch 428/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.3391 - acc: 0.8854 - val_loss: 0.4385 - val_acc: 0.8481\n",
            "Epoch 429/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.3367 - acc: 0.8845 - val_loss: 0.4486 - val_acc: 0.8524\n",
            "Epoch 430/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3325 - acc: 0.8860 - val_loss: 0.4623 - val_acc: 0.8383\n",
            "Epoch 431/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3243 - acc: 0.8932 - val_loss: 0.4411 - val_acc: 0.8579\n",
            "Epoch 432/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3380 - acc: 0.8814 - val_loss: 0.4450 - val_acc: 0.8518\n",
            "Epoch 433/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3140 - acc: 0.8878 - val_loss: 0.4421 - val_acc: 0.8506\n",
            "Epoch 434/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.3203 - acc: 0.8899 - val_loss: 0.4666 - val_acc: 0.8414\n",
            "Epoch 435/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.3273 - acc: 0.8857 - val_loss: 0.4641 - val_acc: 0.8408\n",
            "Epoch 436/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.3347 - acc: 0.8836 - val_loss: 0.4404 - val_acc: 0.8426\n",
            "Epoch 437/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3134 - acc: 0.8938 - val_loss: 0.4511 - val_acc: 0.8445\n",
            "Epoch 438/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3358 - acc: 0.8845 - val_loss: 0.4258 - val_acc: 0.8567\n",
            "Epoch 439/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3224 - acc: 0.8881 - val_loss: 0.4204 - val_acc: 0.8592\n",
            "Epoch 440/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3287 - acc: 0.8884 - val_loss: 0.4346 - val_acc: 0.8573\n",
            "Epoch 441/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.3246 - acc: 0.8908 - val_loss: 0.4423 - val_acc: 0.8506\n",
            "Epoch 442/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3114 - acc: 0.8941 - val_loss: 0.4383 - val_acc: 0.8451\n",
            "Epoch 443/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.3148 - acc: 0.8959 - val_loss: 0.4471 - val_acc: 0.8549\n",
            "Epoch 444/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3123 - acc: 0.8938 - val_loss: 0.4301 - val_acc: 0.8518\n",
            "Epoch 445/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.3221 - acc: 0.8887 - val_loss: 0.4506 - val_acc: 0.8457\n",
            "Epoch 446/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3142 - acc: 0.8932 - val_loss: 0.4815 - val_acc: 0.8249\n",
            "Epoch 447/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.3157 - acc: 0.8875 - val_loss: 0.4200 - val_acc: 0.8567\n",
            "Epoch 448/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.2992 - acc: 0.8989 - val_loss: 0.4234 - val_acc: 0.8610\n",
            "Epoch 449/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3098 - acc: 0.9011 - val_loss: 0.4280 - val_acc: 0.8549\n",
            "Epoch 450/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2976 - acc: 0.8998 - val_loss: 0.4375 - val_acc: 0.8610\n",
            "Epoch 451/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.3225 - acc: 0.8887 - val_loss: 0.4323 - val_acc: 0.8585\n",
            "Epoch 452/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3154 - acc: 0.8914 - val_loss: 0.4154 - val_acc: 0.8598\n",
            "Epoch 453/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2920 - acc: 0.9056 - val_loss: 0.4237 - val_acc: 0.8530\n",
            "Epoch 454/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3055 - acc: 0.8932 - val_loss: 0.4309 - val_acc: 0.8494\n",
            "Epoch 455/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3112 - acc: 0.8911 - val_loss: 0.4620 - val_acc: 0.8383\n",
            "Epoch 456/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3108 - acc: 0.8926 - val_loss: 0.4374 - val_acc: 0.8494\n",
            "Epoch 457/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.3029 - acc: 0.8959 - val_loss: 0.4244 - val_acc: 0.8628\n",
            "Epoch 458/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3152 - acc: 0.8863 - val_loss: 0.4117 - val_acc: 0.8665\n",
            "Epoch 459/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.3082 - acc: 0.8938 - val_loss: 0.4224 - val_acc: 0.8592\n",
            "Epoch 460/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3093 - acc: 0.8974 - val_loss: 0.4321 - val_acc: 0.8506\n",
            "Epoch 461/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3004 - acc: 0.8938 - val_loss: 0.4175 - val_acc: 0.8659\n",
            "Epoch 462/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.3054 - acc: 0.8980 - val_loss: 0.4317 - val_acc: 0.8573\n",
            "Epoch 463/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3065 - acc: 0.8899 - val_loss: 0.4266 - val_acc: 0.8598\n",
            "Epoch 464/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2952 - acc: 0.9011 - val_loss: 0.4196 - val_acc: 0.8610\n",
            "Epoch 465/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3000 - acc: 0.8941 - val_loss: 0.4292 - val_acc: 0.8634\n",
            "Epoch 466/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3014 - acc: 0.8992 - val_loss: 0.4104 - val_acc: 0.8647\n",
            "Epoch 467/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2993 - acc: 0.8992 - val_loss: 0.4204 - val_acc: 0.8628\n",
            "Epoch 468/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.3119 - acc: 0.8911 - val_loss: 0.4101 - val_acc: 0.8598\n",
            "Epoch 469/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2916 - acc: 0.8995 - val_loss: 0.4229 - val_acc: 0.8677\n",
            "Epoch 470/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.3061 - acc: 0.8938 - val_loss: 0.4276 - val_acc: 0.8665\n",
            "Epoch 471/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2959 - acc: 0.9020 - val_loss: 0.3981 - val_acc: 0.8634\n",
            "Epoch 472/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.2853 - acc: 0.9044 - val_loss: 0.4116 - val_acc: 0.8585\n",
            "Epoch 473/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.3030 - acc: 0.8956 - val_loss: 0.4286 - val_acc: 0.8628\n",
            "Epoch 474/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2910 - acc: 0.8989 - val_loss: 0.4112 - val_acc: 0.8653\n",
            "Epoch 475/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2955 - acc: 0.8992 - val_loss: 0.4361 - val_acc: 0.8457\n",
            "Epoch 476/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2821 - acc: 0.9056 - val_loss: 0.4369 - val_acc: 0.8512\n",
            "Epoch 477/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2910 - acc: 0.9029 - val_loss: 0.4145 - val_acc: 0.8671\n",
            "Epoch 478/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2949 - acc: 0.9011 - val_loss: 0.3918 - val_acc: 0.8720\n",
            "Epoch 479/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2912 - acc: 0.9059 - val_loss: 0.4144 - val_acc: 0.8634\n",
            "Epoch 480/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2937 - acc: 0.8998 - val_loss: 0.4189 - val_acc: 0.8524\n",
            "Epoch 481/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.3031 - acc: 0.8974 - val_loss: 0.4365 - val_acc: 0.8481\n",
            "Epoch 482/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.2820 - acc: 0.9026 - val_loss: 0.4147 - val_acc: 0.8653\n",
            "Epoch 483/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2924 - acc: 0.9029 - val_loss: 0.4067 - val_acc: 0.8683\n",
            "Epoch 484/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2859 - acc: 0.9032 - val_loss: 0.4188 - val_acc: 0.8604\n",
            "Epoch 485/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2904 - acc: 0.8962 - val_loss: 0.4170 - val_acc: 0.8604\n",
            "Epoch 486/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2791 - acc: 0.9050 - val_loss: 0.4183 - val_acc: 0.8714\n",
            "Epoch 487/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.2704 - acc: 0.9059 - val_loss: 0.4237 - val_acc: 0.8555\n",
            "Epoch 488/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2900 - acc: 0.9029 - val_loss: 0.4322 - val_acc: 0.8500\n",
            "Epoch 489/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2919 - acc: 0.9041 - val_loss: 0.4387 - val_acc: 0.8494\n",
            "Epoch 490/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2884 - acc: 0.8989 - val_loss: 0.4143 - val_acc: 0.8561\n",
            "Epoch 491/1000\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.2872 - acc: 0.9023 - val_loss: 0.4286 - val_acc: 0.8567\n",
            "Epoch 492/1000\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.2701 - acc: 0.9062 - val_loss: 0.4232 - val_acc: 0.8543\n",
            "Epoch 493/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2744 - acc: 0.9095 - val_loss: 0.4167 - val_acc: 0.8567\n",
            "Epoch 494/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.2774 - acc: 0.9011 - val_loss: 0.3958 - val_acc: 0.8671\n",
            "Epoch 495/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.2877 - acc: 0.9065 - val_loss: 0.4454 - val_acc: 0.8487\n",
            "Epoch 496/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2720 - acc: 0.9071 - val_loss: 0.4292 - val_acc: 0.8500\n",
            "Epoch 497/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2778 - acc: 0.9044 - val_loss: 0.3854 - val_acc: 0.8708\n",
            "Epoch 498/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.2623 - acc: 0.9074 - val_loss: 0.3909 - val_acc: 0.8757\n",
            "Epoch 499/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2863 - acc: 0.9005 - val_loss: 0.4142 - val_acc: 0.8616\n",
            "Epoch 500/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2749 - acc: 0.9029 - val_loss: 0.3964 - val_acc: 0.8677\n",
            "Epoch 501/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2856 - acc: 0.8995 - val_loss: 0.4111 - val_acc: 0.8641\n",
            "Epoch 502/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2760 - acc: 0.9077 - val_loss: 0.4027 - val_acc: 0.8573\n",
            "Epoch 503/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2617 - acc: 0.9137 - val_loss: 0.4150 - val_acc: 0.8671\n",
            "Epoch 504/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2739 - acc: 0.9095 - val_loss: 0.4157 - val_acc: 0.8585\n",
            "Epoch 505/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.2708 - acc: 0.9086 - val_loss: 0.4069 - val_acc: 0.8628\n",
            "Epoch 506/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2668 - acc: 0.9089 - val_loss: 0.3972 - val_acc: 0.8659\n",
            "Epoch 507/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2749 - acc: 0.9041 - val_loss: 0.4151 - val_acc: 0.8641\n",
            "Epoch 508/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2680 - acc: 0.9092 - val_loss: 0.4300 - val_acc: 0.8500\n",
            "Epoch 509/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2797 - acc: 0.9002 - val_loss: 0.3947 - val_acc: 0.8739\n",
            "Epoch 510/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2687 - acc: 0.9101 - val_loss: 0.4023 - val_acc: 0.8579\n",
            "Epoch 511/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2564 - acc: 0.9125 - val_loss: 0.4177 - val_acc: 0.8610\n",
            "Epoch 512/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.2737 - acc: 0.9080 - val_loss: 0.4135 - val_acc: 0.8622\n",
            "Epoch 513/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2590 - acc: 0.9131 - val_loss: 0.4075 - val_acc: 0.8543\n",
            "Epoch 514/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.2745 - acc: 0.8995 - val_loss: 0.3823 - val_acc: 0.8720\n",
            "Epoch 515/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.2628 - acc: 0.9083 - val_loss: 0.4249 - val_acc: 0.8549\n",
            "Epoch 516/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2813 - acc: 0.9053 - val_loss: 0.3901 - val_acc: 0.8745\n",
            "Epoch 517/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2559 - acc: 0.9149 - val_loss: 0.3958 - val_acc: 0.8739\n",
            "Epoch 518/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2711 - acc: 0.9044 - val_loss: 0.4072 - val_acc: 0.8696\n",
            "Epoch 519/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.2628 - acc: 0.9068 - val_loss: 0.3950 - val_acc: 0.8788\n",
            "Epoch 520/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2630 - acc: 0.8974 - val_loss: 0.4234 - val_acc: 0.8579\n",
            "Epoch 521/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2589 - acc: 0.9077 - val_loss: 0.4140 - val_acc: 0.8628\n",
            "Epoch 522/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.2437 - acc: 0.9210 - val_loss: 0.3882 - val_acc: 0.8781\n",
            "Epoch 523/1000\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.2698 - acc: 0.9074 - val_loss: 0.3972 - val_acc: 0.8763\n",
            "Epoch 524/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2681 - acc: 0.9068 - val_loss: 0.4076 - val_acc: 0.8739\n",
            "Epoch 525/1000\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.2703 - acc: 0.9092 - val_loss: 0.4183 - val_acc: 0.8524\n",
            "Epoch 526/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.2581 - acc: 0.9149 - val_loss: 0.3829 - val_acc: 0.8775\n",
            "Epoch 527/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2578 - acc: 0.9113 - val_loss: 0.3999 - val_acc: 0.8732\n",
            "Epoch 528/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2567 - acc: 0.9125 - val_loss: 0.3963 - val_acc: 0.8726\n",
            "Epoch 529/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2624 - acc: 0.9071 - val_loss: 0.3820 - val_acc: 0.8745\n",
            "Epoch 530/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2578 - acc: 0.9176 - val_loss: 0.4107 - val_acc: 0.8598\n",
            "Epoch 531/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.2528 - acc: 0.9155 - val_loss: 0.3849 - val_acc: 0.8671\n",
            "Epoch 532/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2503 - acc: 0.9176 - val_loss: 0.3788 - val_acc: 0.8769\n",
            "Epoch 533/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2446 - acc: 0.9176 - val_loss: 0.3851 - val_acc: 0.8775\n",
            "Epoch 534/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2530 - acc: 0.9128 - val_loss: 0.3757 - val_acc: 0.8788\n",
            "Epoch 535/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.2590 - acc: 0.9152 - val_loss: 0.3877 - val_acc: 0.8714\n",
            "Epoch 536/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2434 - acc: 0.9173 - val_loss: 0.3925 - val_acc: 0.8757\n",
            "Epoch 537/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2642 - acc: 0.9065 - val_loss: 0.3906 - val_acc: 0.8855\n",
            "Epoch 538/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2484 - acc: 0.9164 - val_loss: 0.3883 - val_acc: 0.8726\n",
            "Epoch 539/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2464 - acc: 0.9101 - val_loss: 0.3739 - val_acc: 0.8739\n",
            "Epoch 540/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2343 - acc: 0.9207 - val_loss: 0.3894 - val_acc: 0.8726\n",
            "Epoch 541/1000\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.2413 - acc: 0.9143 - val_loss: 0.4128 - val_acc: 0.8702\n",
            "Epoch 542/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2342 - acc: 0.9228 - val_loss: 0.4248 - val_acc: 0.8585\n",
            "Epoch 543/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2576 - acc: 0.9131 - val_loss: 0.3978 - val_acc: 0.8690\n",
            "Epoch 544/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.2613 - acc: 0.9107 - val_loss: 0.3732 - val_acc: 0.8843\n",
            "Epoch 545/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2560 - acc: 0.9131 - val_loss: 0.3902 - val_acc: 0.8781\n",
            "Epoch 546/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2437 - acc: 0.9134 - val_loss: 0.3713 - val_acc: 0.8794\n",
            "Epoch 547/1000\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.2300 - acc: 0.9219 - val_loss: 0.3974 - val_acc: 0.8788\n",
            "Epoch 548/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.2537 - acc: 0.9149 - val_loss: 0.3749 - val_acc: 0.8763\n",
            "Epoch 549/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2318 - acc: 0.9170 - val_loss: 0.3919 - val_acc: 0.8775\n",
            "Epoch 550/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2404 - acc: 0.9219 - val_loss: 0.3901 - val_acc: 0.8677\n",
            "Epoch 551/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.2311 - acc: 0.9222 - val_loss: 0.4094 - val_acc: 0.8683\n",
            "Epoch 552/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2486 - acc: 0.9122 - val_loss: 0.4122 - val_acc: 0.8732\n",
            "Epoch 553/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2390 - acc: 0.9219 - val_loss: 0.3716 - val_acc: 0.8824\n",
            "Epoch 554/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2354 - acc: 0.9258 - val_loss: 0.3916 - val_acc: 0.8708\n",
            "Epoch 555/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2505 - acc: 0.9113 - val_loss: 0.3757 - val_acc: 0.8824\n",
            "Epoch 556/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2364 - acc: 0.9164 - val_loss: 0.3785 - val_acc: 0.8824\n",
            "Epoch 557/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2438 - acc: 0.9158 - val_loss: 0.3774 - val_acc: 0.8788\n",
            "Epoch 558/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2343 - acc: 0.9201 - val_loss: 0.3944 - val_acc: 0.8665\n",
            "Epoch 559/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2352 - acc: 0.9195 - val_loss: 0.3768 - val_acc: 0.8873\n",
            "Epoch 560/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.2386 - acc: 0.9173 - val_loss: 0.3958 - val_acc: 0.8616\n",
            "Epoch 561/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2417 - acc: 0.9195 - val_loss: 0.3603 - val_acc: 0.8830\n",
            "Epoch 562/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2380 - acc: 0.9149 - val_loss: 0.3676 - val_acc: 0.8824\n",
            "Epoch 563/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.2306 - acc: 0.9225 - val_loss: 0.3919 - val_acc: 0.8641\n",
            "Epoch 564/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2433 - acc: 0.9173 - val_loss: 0.3721 - val_acc: 0.8818\n",
            "Epoch 565/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2456 - acc: 0.9161 - val_loss: 0.3804 - val_acc: 0.8861\n",
            "Epoch 566/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2343 - acc: 0.9204 - val_loss: 0.3681 - val_acc: 0.8824\n",
            "Epoch 567/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2381 - acc: 0.9192 - val_loss: 0.3926 - val_acc: 0.8683\n",
            "Epoch 568/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2320 - acc: 0.9186 - val_loss: 0.3704 - val_acc: 0.8873\n",
            "Epoch 569/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2396 - acc: 0.9170 - val_loss: 0.3714 - val_acc: 0.8806\n",
            "Epoch 570/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2279 - acc: 0.9183 - val_loss: 0.3617 - val_acc: 0.8879\n",
            "Epoch 571/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2402 - acc: 0.9155 - val_loss: 0.4021 - val_acc: 0.8671\n",
            "Epoch 572/1000\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.2308 - acc: 0.9240 - val_loss: 0.3607 - val_acc: 0.8818\n",
            "Epoch 573/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.2366 - acc: 0.9186 - val_loss: 0.3740 - val_acc: 0.8824\n",
            "Epoch 574/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2232 - acc: 0.9173 - val_loss: 0.3701 - val_acc: 0.8824\n",
            "Epoch 575/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2266 - acc: 0.9207 - val_loss: 0.3638 - val_acc: 0.8830\n",
            "Epoch 576/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2348 - acc: 0.9192 - val_loss: 0.3646 - val_acc: 0.8855\n",
            "Epoch 577/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.2258 - acc: 0.9300 - val_loss: 0.3598 - val_acc: 0.8812\n",
            "Epoch 578/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2246 - acc: 0.9216 - val_loss: 0.3684 - val_acc: 0.8910\n",
            "Epoch 579/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2324 - acc: 0.9234 - val_loss: 0.3791 - val_acc: 0.8830\n",
            "Epoch 580/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2275 - acc: 0.9216 - val_loss: 0.4086 - val_acc: 0.8702\n",
            "Epoch 581/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2276 - acc: 0.9192 - val_loss: 0.3843 - val_acc: 0.8763\n",
            "Epoch 582/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2326 - acc: 0.9192 - val_loss: 0.3768 - val_acc: 0.8794\n",
            "Epoch 583/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.2241 - acc: 0.9240 - val_loss: 0.3679 - val_acc: 0.8867\n",
            "Epoch 584/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2431 - acc: 0.9189 - val_loss: 0.3729 - val_acc: 0.8751\n",
            "Epoch 585/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2140 - acc: 0.9282 - val_loss: 0.3696 - val_acc: 0.8824\n",
            "Epoch 586/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2228 - acc: 0.9210 - val_loss: 0.3853 - val_acc: 0.8781\n",
            "Epoch 587/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2208 - acc: 0.9273 - val_loss: 0.3682 - val_acc: 0.8806\n",
            "Epoch 588/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2171 - acc: 0.9255 - val_loss: 0.3524 - val_acc: 0.8892\n",
            "Epoch 589/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2205 - acc: 0.9273 - val_loss: 0.3414 - val_acc: 0.8879\n",
            "Epoch 590/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.2310 - acc: 0.9179 - val_loss: 0.3754 - val_acc: 0.8818\n",
            "Epoch 591/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.2237 - acc: 0.9195 - val_loss: 0.3650 - val_acc: 0.8861\n",
            "Epoch 592/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2379 - acc: 0.9125 - val_loss: 0.3793 - val_acc: 0.8781\n",
            "Epoch 593/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.2189 - acc: 0.9222 - val_loss: 0.3630 - val_acc: 0.8788\n",
            "Epoch 594/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2150 - acc: 0.9258 - val_loss: 0.3657 - val_acc: 0.8855\n",
            "Epoch 595/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.2213 - acc: 0.9258 - val_loss: 0.3799 - val_acc: 0.8745\n",
            "Epoch 596/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2152 - acc: 0.9273 - val_loss: 0.3699 - val_acc: 0.8904\n",
            "Epoch 597/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.2127 - acc: 0.9261 - val_loss: 0.3817 - val_acc: 0.8757\n",
            "Epoch 598/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2136 - acc: 0.9291 - val_loss: 0.3835 - val_acc: 0.8757\n",
            "Epoch 599/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.2167 - acc: 0.9258 - val_loss: 0.3611 - val_acc: 0.8904\n",
            "Epoch 600/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2076 - acc: 0.9342 - val_loss: 0.3617 - val_acc: 0.8818\n",
            "Epoch 601/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2358 - acc: 0.9189 - val_loss: 0.3562 - val_acc: 0.8861\n",
            "Epoch 602/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2100 - acc: 0.9291 - val_loss: 0.3647 - val_acc: 0.8873\n",
            "Epoch 603/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2180 - acc: 0.9279 - val_loss: 0.3678 - val_acc: 0.8763\n",
            "Epoch 604/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2219 - acc: 0.9243 - val_loss: 0.3790 - val_acc: 0.8763\n",
            "Epoch 605/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2110 - acc: 0.9285 - val_loss: 0.3724 - val_acc: 0.8843\n",
            "Epoch 606/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2151 - acc: 0.9285 - val_loss: 0.3838 - val_acc: 0.8843\n",
            "Epoch 607/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.1959 - acc: 0.9342 - val_loss: 0.3469 - val_acc: 0.8873\n",
            "Epoch 608/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2166 - acc: 0.9225 - val_loss: 0.3745 - val_acc: 0.8751\n",
            "Epoch 609/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2075 - acc: 0.9330 - val_loss: 0.3638 - val_acc: 0.8830\n",
            "Epoch 610/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2123 - acc: 0.9252 - val_loss: 0.3538 - val_acc: 0.8910\n",
            "Epoch 611/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.2155 - acc: 0.9252 - val_loss: 0.3653 - val_acc: 0.8861\n",
            "Epoch 612/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2010 - acc: 0.9339 - val_loss: 0.3740 - val_acc: 0.8836\n",
            "Epoch 613/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2295 - acc: 0.9225 - val_loss: 0.3441 - val_acc: 0.9008\n",
            "Epoch 614/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2033 - acc: 0.9318 - val_loss: 0.3674 - val_acc: 0.8775\n",
            "Epoch 615/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2162 - acc: 0.9240 - val_loss: 0.3410 - val_acc: 0.9014\n",
            "Epoch 616/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.1990 - acc: 0.9315 - val_loss: 0.3588 - val_acc: 0.8910\n",
            "Epoch 617/1000\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.2047 - acc: 0.9300 - val_loss: 0.3540 - val_acc: 0.8855\n",
            "Epoch 618/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2148 - acc: 0.9261 - val_loss: 0.3789 - val_acc: 0.8824\n",
            "Epoch 619/1000\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.2157 - acc: 0.9267 - val_loss: 0.3615 - val_acc: 0.8904\n",
            "Epoch 620/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2175 - acc: 0.9258 - val_loss: 0.3652 - val_acc: 0.8867\n",
            "Epoch 621/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.2046 - acc: 0.9388 - val_loss: 0.3468 - val_acc: 0.8941\n",
            "Epoch 622/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.2087 - acc: 0.9309 - val_loss: 0.3734 - val_acc: 0.8824\n",
            "Epoch 623/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2168 - acc: 0.9252 - val_loss: 0.3523 - val_acc: 0.8873\n",
            "Epoch 624/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2113 - acc: 0.9306 - val_loss: 0.3997 - val_acc: 0.8708\n",
            "Epoch 625/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2173 - acc: 0.9255 - val_loss: 0.3524 - val_acc: 0.8941\n",
            "Epoch 626/1000\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.2095 - acc: 0.9282 - val_loss: 0.3508 - val_acc: 0.8904\n",
            "Epoch 627/1000\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.1902 - acc: 0.9376 - val_loss: 0.3527 - val_acc: 0.8904\n",
            "Epoch 628/1000\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.2125 - acc: 0.9276 - val_loss: 0.3565 - val_acc: 0.8879\n",
            "Epoch 629/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.1919 - acc: 0.9321 - val_loss: 0.3340 - val_acc: 0.8916\n",
            "Epoch 630/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2075 - acc: 0.9303 - val_loss: 0.3589 - val_acc: 0.8977\n",
            "Epoch 631/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2089 - acc: 0.9303 - val_loss: 0.3406 - val_acc: 0.8934\n",
            "Epoch 632/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2003 - acc: 0.9367 - val_loss: 0.3429 - val_acc: 0.8959\n",
            "Epoch 633/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2011 - acc: 0.9285 - val_loss: 0.3607 - val_acc: 0.8861\n",
            "Epoch 634/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2017 - acc: 0.9306 - val_loss: 0.3583 - val_acc: 0.8885\n",
            "Epoch 635/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2129 - acc: 0.9276 - val_loss: 0.3282 - val_acc: 0.8941\n",
            "Epoch 636/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2009 - acc: 0.9303 - val_loss: 0.3544 - val_acc: 0.8965\n",
            "Epoch 637/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.2105 - acc: 0.9261 - val_loss: 0.3590 - val_acc: 0.8824\n",
            "Epoch 638/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.1968 - acc: 0.9306 - val_loss: 0.3667 - val_acc: 0.8867\n",
            "Epoch 639/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2130 - acc: 0.9237 - val_loss: 0.3559 - val_acc: 0.8885\n",
            "Epoch 640/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2012 - acc: 0.9321 - val_loss: 0.3581 - val_acc: 0.8867\n",
            "Epoch 641/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.2031 - acc: 0.9324 - val_loss: 0.3828 - val_acc: 0.8824\n",
            "Epoch 642/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1993 - acc: 0.9309 - val_loss: 0.3443 - val_acc: 0.8965\n",
            "Epoch 643/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.2061 - acc: 0.9309 - val_loss: 0.3485 - val_acc: 0.8922\n",
            "Epoch 644/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.2116 - acc: 0.9246 - val_loss: 0.3415 - val_acc: 0.8965\n",
            "Epoch 645/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1987 - acc: 0.9270 - val_loss: 0.3662 - val_acc: 0.8867\n",
            "Epoch 646/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1919 - acc: 0.9385 - val_loss: 0.3493 - val_acc: 0.8867\n",
            "Epoch 647/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.1976 - acc: 0.9309 - val_loss: 0.3502 - val_acc: 0.8934\n",
            "Epoch 648/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.1881 - acc: 0.9336 - val_loss: 0.3778 - val_acc: 0.8849\n",
            "Epoch 649/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.1852 - acc: 0.9415 - val_loss: 0.3408 - val_acc: 0.8910\n",
            "Epoch 650/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.1826 - acc: 0.9373 - val_loss: 0.3537 - val_acc: 0.8867\n",
            "Epoch 651/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1945 - acc: 0.9339 - val_loss: 0.3809 - val_acc: 0.8824\n",
            "Epoch 652/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.2005 - acc: 0.9276 - val_loss: 0.3455 - val_acc: 0.8922\n",
            "Epoch 653/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1967 - acc: 0.9318 - val_loss: 0.3449 - val_acc: 0.9014\n",
            "Epoch 654/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.2101 - acc: 0.9282 - val_loss: 0.3408 - val_acc: 0.9026\n",
            "Epoch 655/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.2079 - acc: 0.9279 - val_loss: 0.3343 - val_acc: 0.8983\n",
            "Epoch 656/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1959 - acc: 0.9321 - val_loss: 0.3381 - val_acc: 0.8983\n",
            "Epoch 657/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.2130 - acc: 0.9276 - val_loss: 0.3614 - val_acc: 0.8892\n",
            "Epoch 658/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1930 - acc: 0.9342 - val_loss: 0.3575 - val_acc: 0.8910\n",
            "Epoch 659/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.1833 - acc: 0.9391 - val_loss: 0.3434 - val_acc: 0.8849\n",
            "Epoch 660/1000\n",
            "3315/3315 [==============================] - 1s 451us/step - loss: 0.1956 - acc: 0.9339 - val_loss: 0.3581 - val_acc: 0.8885\n",
            "Epoch 661/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1929 - acc: 0.9315 - val_loss: 0.3516 - val_acc: 0.8983\n",
            "Epoch 662/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.2019 - acc: 0.9318 - val_loss: 0.3436 - val_acc: 0.8928\n",
            "Epoch 663/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1859 - acc: 0.9351 - val_loss: 0.3402 - val_acc: 0.8947\n",
            "Epoch 664/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1812 - acc: 0.9400 - val_loss: 0.3472 - val_acc: 0.8928\n",
            "Epoch 665/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1944 - acc: 0.9382 - val_loss: 0.3295 - val_acc: 0.9026\n",
            "Epoch 666/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1860 - acc: 0.9354 - val_loss: 0.3492 - val_acc: 0.8910\n",
            "Epoch 667/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1885 - acc: 0.9370 - val_loss: 0.3462 - val_acc: 0.8922\n",
            "Epoch 668/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.1947 - acc: 0.9370 - val_loss: 0.3367 - val_acc: 0.9075\n",
            "Epoch 669/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1918 - acc: 0.9376 - val_loss: 0.3290 - val_acc: 0.9008\n",
            "Epoch 670/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1975 - acc: 0.9342 - val_loss: 0.3387 - val_acc: 0.8965\n",
            "Epoch 671/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.1807 - acc: 0.9391 - val_loss: 0.3568 - val_acc: 0.8947\n",
            "Epoch 672/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.1831 - acc: 0.9400 - val_loss: 0.3528 - val_acc: 0.8892\n",
            "Epoch 673/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.1931 - acc: 0.9354 - val_loss: 0.3571 - val_acc: 0.8898\n",
            "Epoch 674/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1857 - acc: 0.9424 - val_loss: 0.3535 - val_acc: 0.8867\n",
            "Epoch 675/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.1764 - acc: 0.9451 - val_loss: 0.3390 - val_acc: 0.8953\n",
            "Epoch 676/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1892 - acc: 0.9376 - val_loss: 0.3455 - val_acc: 0.8965\n",
            "Epoch 677/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1882 - acc: 0.9397 - val_loss: 0.3274 - val_acc: 0.8983\n",
            "Epoch 678/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1748 - acc: 0.9442 - val_loss: 0.3361 - val_acc: 0.9032\n",
            "Epoch 679/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1742 - acc: 0.9436 - val_loss: 0.3378 - val_acc: 0.9057\n",
            "Epoch 680/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1867 - acc: 0.9351 - val_loss: 0.3416 - val_acc: 0.9039\n",
            "Epoch 681/1000\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.1841 - acc: 0.9336 - val_loss: 0.3276 - val_acc: 0.9039\n",
            "Epoch 682/1000\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.1907 - acc: 0.9357 - val_loss: 0.3437 - val_acc: 0.9020\n",
            "Epoch 683/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.1868 - acc: 0.9409 - val_loss: 0.3248 - val_acc: 0.9008\n",
            "Epoch 684/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1764 - acc: 0.9382 - val_loss: 0.3384 - val_acc: 0.8977\n",
            "Epoch 685/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1795 - acc: 0.9345 - val_loss: 0.3480 - val_acc: 0.8904\n",
            "Epoch 686/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1866 - acc: 0.9318 - val_loss: 0.3518 - val_acc: 0.8990\n",
            "Epoch 687/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.1776 - acc: 0.9403 - val_loss: 0.3493 - val_acc: 0.8971\n",
            "Epoch 688/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1906 - acc: 0.9348 - val_loss: 0.3470 - val_acc: 0.8885\n",
            "Epoch 689/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.1754 - acc: 0.9397 - val_loss: 0.3273 - val_acc: 0.9045\n",
            "Epoch 690/1000\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.1835 - acc: 0.9382 - val_loss: 0.3310 - val_acc: 0.9020\n",
            "Epoch 691/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1840 - acc: 0.9388 - val_loss: 0.3418 - val_acc: 0.9002\n",
            "Epoch 692/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1800 - acc: 0.9394 - val_loss: 0.3477 - val_acc: 0.9002\n",
            "Epoch 693/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1891 - acc: 0.9418 - val_loss: 0.3416 - val_acc: 0.8941\n",
            "Epoch 694/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1819 - acc: 0.9373 - val_loss: 0.3278 - val_acc: 0.8953\n",
            "Epoch 695/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1834 - acc: 0.9382 - val_loss: 0.3343 - val_acc: 0.8941\n",
            "Epoch 696/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1833 - acc: 0.9412 - val_loss: 0.3486 - val_acc: 0.8996\n",
            "Epoch 697/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.1750 - acc: 0.9442 - val_loss: 0.3354 - val_acc: 0.8996\n",
            "Epoch 698/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.1712 - acc: 0.9421 - val_loss: 0.3210 - val_acc: 0.9002\n",
            "Epoch 699/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.1816 - acc: 0.9388 - val_loss: 0.3200 - val_acc: 0.9045\n",
            "Epoch 700/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.1797 - acc: 0.9400 - val_loss: 0.3371 - val_acc: 0.8959\n",
            "Epoch 701/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1727 - acc: 0.9357 - val_loss: 0.3625 - val_acc: 0.8898\n",
            "Epoch 702/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1816 - acc: 0.9363 - val_loss: 0.3274 - val_acc: 0.8959\n",
            "Epoch 703/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1753 - acc: 0.9427 - val_loss: 0.3261 - val_acc: 0.9026\n",
            "Epoch 704/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1864 - acc: 0.9382 - val_loss: 0.3517 - val_acc: 0.8904\n",
            "Epoch 705/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1911 - acc: 0.9373 - val_loss: 0.3392 - val_acc: 0.8965\n",
            "Epoch 706/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1838 - acc: 0.9360 - val_loss: 0.3361 - val_acc: 0.9014\n",
            "Epoch 707/1000\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.1648 - acc: 0.9397 - val_loss: 0.3478 - val_acc: 0.8990\n",
            "Epoch 708/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.1792 - acc: 0.9421 - val_loss: 0.3496 - val_acc: 0.8977\n",
            "Epoch 709/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.1838 - acc: 0.9363 - val_loss: 0.3249 - val_acc: 0.9045\n",
            "Epoch 710/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1864 - acc: 0.9348 - val_loss: 0.3328 - val_acc: 0.9051\n",
            "Epoch 711/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1744 - acc: 0.9379 - val_loss: 0.3103 - val_acc: 0.9094\n",
            "Epoch 712/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1764 - acc: 0.9409 - val_loss: 0.3433 - val_acc: 0.8990\n",
            "Epoch 713/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1841 - acc: 0.9370 - val_loss: 0.3281 - val_acc: 0.9014\n",
            "Epoch 714/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.1689 - acc: 0.9439 - val_loss: 0.3336 - val_acc: 0.8922\n",
            "Epoch 715/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.1672 - acc: 0.9442 - val_loss: 0.3338 - val_acc: 0.9026\n",
            "Epoch 716/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1696 - acc: 0.9412 - val_loss: 0.3375 - val_acc: 0.8977\n",
            "Epoch 717/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.1721 - acc: 0.9385 - val_loss: 0.3364 - val_acc: 0.9057\n",
            "Epoch 718/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.1607 - acc: 0.9496 - val_loss: 0.3409 - val_acc: 0.9002\n",
            "Epoch 719/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1791 - acc: 0.9376 - val_loss: 0.3308 - val_acc: 0.8990\n",
            "Epoch 720/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.1676 - acc: 0.9430 - val_loss: 0.3260 - val_acc: 0.9051\n",
            "Epoch 721/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1630 - acc: 0.9454 - val_loss: 0.3315 - val_acc: 0.9014\n",
            "Epoch 722/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1799 - acc: 0.9363 - val_loss: 0.3417 - val_acc: 0.9002\n",
            "Epoch 723/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.1683 - acc: 0.9412 - val_loss: 0.3361 - val_acc: 0.8910\n",
            "Epoch 724/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1683 - acc: 0.9394 - val_loss: 0.3309 - val_acc: 0.8990\n",
            "Epoch 725/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1786 - acc: 0.9354 - val_loss: 0.3417 - val_acc: 0.9008\n",
            "Epoch 726/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.1709 - acc: 0.9448 - val_loss: 0.3541 - val_acc: 0.8983\n",
            "Epoch 727/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.1584 - acc: 0.9475 - val_loss: 0.3374 - val_acc: 0.8996\n",
            "Epoch 728/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1718 - acc: 0.9463 - val_loss: 0.3430 - val_acc: 0.8977\n",
            "Epoch 729/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.1687 - acc: 0.9421 - val_loss: 0.3511 - val_acc: 0.8898\n",
            "Epoch 730/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1706 - acc: 0.9466 - val_loss: 0.3622 - val_acc: 0.8916\n",
            "Epoch 731/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.1756 - acc: 0.9421 - val_loss: 0.3365 - val_acc: 0.8996\n",
            "Epoch 732/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.1679 - acc: 0.9400 - val_loss: 0.3311 - val_acc: 0.9069\n",
            "Epoch 733/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1846 - acc: 0.9370 - val_loss: 0.3150 - val_acc: 0.9088\n",
            "Epoch 734/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1668 - acc: 0.9400 - val_loss: 0.3440 - val_acc: 0.8959\n",
            "Epoch 735/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1558 - acc: 0.9460 - val_loss: 0.3431 - val_acc: 0.8922\n",
            "Epoch 736/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1818 - acc: 0.9376 - val_loss: 0.3280 - val_acc: 0.8990\n",
            "Epoch 737/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1703 - acc: 0.9439 - val_loss: 0.3511 - val_acc: 0.9002\n",
            "Epoch 738/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1693 - acc: 0.9451 - val_loss: 0.3426 - val_acc: 0.8996\n",
            "Epoch 739/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1625 - acc: 0.9412 - val_loss: 0.3262 - val_acc: 0.9063\n",
            "Epoch 740/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1756 - acc: 0.9430 - val_loss: 0.3433 - val_acc: 0.8983\n",
            "Epoch 741/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1687 - acc: 0.9427 - val_loss: 0.3305 - val_acc: 0.9137\n",
            "Epoch 742/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1603 - acc: 0.9466 - val_loss: 0.3407 - val_acc: 0.9008\n",
            "Epoch 743/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.1750 - acc: 0.9412 - val_loss: 0.3399 - val_acc: 0.8996\n",
            "Epoch 744/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1547 - acc: 0.9496 - val_loss: 0.3570 - val_acc: 0.8941\n",
            "Epoch 745/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1604 - acc: 0.9427 - val_loss: 0.3238 - val_acc: 0.9057\n",
            "Epoch 746/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.1643 - acc: 0.9460 - val_loss: 0.3320 - val_acc: 0.9008\n",
            "Epoch 747/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1666 - acc: 0.9445 - val_loss: 0.3470 - val_acc: 0.8971\n",
            "Epoch 748/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1717 - acc: 0.9421 - val_loss: 0.3335 - val_acc: 0.8971\n",
            "Epoch 749/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1501 - acc: 0.9538 - val_loss: 0.3183 - val_acc: 0.9069\n",
            "Epoch 750/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1599 - acc: 0.9427 - val_loss: 0.3240 - val_acc: 0.8990\n",
            "Epoch 751/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1601 - acc: 0.9487 - val_loss: 0.3317 - val_acc: 0.8947\n",
            "Epoch 752/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1642 - acc: 0.9424 - val_loss: 0.3142 - val_acc: 0.9081\n",
            "Epoch 753/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1598 - acc: 0.9466 - val_loss: 0.3027 - val_acc: 0.9081\n",
            "Epoch 754/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1500 - acc: 0.9499 - val_loss: 0.3181 - val_acc: 0.9051\n",
            "Epoch 755/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1525 - acc: 0.9544 - val_loss: 0.3371 - val_acc: 0.9014\n",
            "Epoch 756/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1629 - acc: 0.9466 - val_loss: 0.3239 - val_acc: 0.9094\n",
            "Epoch 757/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1640 - acc: 0.9415 - val_loss: 0.3213 - val_acc: 0.9057\n",
            "Epoch 758/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1590 - acc: 0.9487 - val_loss: 0.3256 - val_acc: 0.9069\n",
            "Epoch 759/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1571 - acc: 0.9469 - val_loss: 0.3257 - val_acc: 0.9002\n",
            "Epoch 760/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1552 - acc: 0.9481 - val_loss: 0.3393 - val_acc: 0.9020\n",
            "Epoch 761/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1571 - acc: 0.9445 - val_loss: 0.3387 - val_acc: 0.9069\n",
            "Epoch 762/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1561 - acc: 0.9493 - val_loss: 0.3423 - val_acc: 0.8947\n",
            "Epoch 763/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1583 - acc: 0.9475 - val_loss: 0.3356 - val_acc: 0.8953\n",
            "Epoch 764/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1586 - acc: 0.9478 - val_loss: 0.3190 - val_acc: 0.9143\n",
            "Epoch 765/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1633 - acc: 0.9454 - val_loss: 0.3214 - val_acc: 0.9088\n",
            "Epoch 766/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1648 - acc: 0.9448 - val_loss: 0.3220 - val_acc: 0.9106\n",
            "Epoch 767/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1513 - acc: 0.9517 - val_loss: 0.3393 - val_acc: 0.8904\n",
            "Epoch 768/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1585 - acc: 0.9490 - val_loss: 0.3173 - val_acc: 0.9039\n",
            "Epoch 769/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1613 - acc: 0.9433 - val_loss: 0.3156 - val_acc: 0.9069\n",
            "Epoch 770/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1544 - acc: 0.9511 - val_loss: 0.3311 - val_acc: 0.9057\n",
            "Epoch 771/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1595 - acc: 0.9433 - val_loss: 0.3322 - val_acc: 0.8971\n",
            "Epoch 772/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.1600 - acc: 0.9424 - val_loss: 0.3573 - val_acc: 0.8922\n",
            "Epoch 773/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.1551 - acc: 0.9445 - val_loss: 0.3408 - val_acc: 0.8953\n",
            "Epoch 774/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1620 - acc: 0.9436 - val_loss: 0.3151 - val_acc: 0.9032\n",
            "Epoch 775/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1540 - acc: 0.9490 - val_loss: 0.3513 - val_acc: 0.8904\n",
            "Epoch 776/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1623 - acc: 0.9469 - val_loss: 0.3180 - val_acc: 0.9075\n",
            "Epoch 777/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.1575 - acc: 0.9424 - val_loss: 0.3589 - val_acc: 0.8904\n",
            "Epoch 778/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1502 - acc: 0.9466 - val_loss: 0.3449 - val_acc: 0.9045\n",
            "Epoch 779/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1550 - acc: 0.9475 - val_loss: 0.3202 - val_acc: 0.9057\n",
            "Epoch 780/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1537 - acc: 0.9484 - val_loss: 0.3159 - val_acc: 0.9063\n",
            "Epoch 781/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1662 - acc: 0.9433 - val_loss: 0.3055 - val_acc: 0.9063\n",
            "Epoch 782/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1626 - acc: 0.9484 - val_loss: 0.3273 - val_acc: 0.9008\n",
            "Epoch 783/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1520 - acc: 0.9484 - val_loss: 0.3295 - val_acc: 0.8996\n",
            "Epoch 784/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1450 - acc: 0.9526 - val_loss: 0.3196 - val_acc: 0.9032\n",
            "Epoch 785/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1469 - acc: 0.9487 - val_loss: 0.3666 - val_acc: 0.9002\n",
            "Epoch 786/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1444 - acc: 0.9487 - val_loss: 0.3411 - val_acc: 0.9020\n",
            "Epoch 787/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1512 - acc: 0.9463 - val_loss: 0.3247 - val_acc: 0.9039\n",
            "Epoch 788/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1452 - acc: 0.9505 - val_loss: 0.3075 - val_acc: 0.9026\n",
            "Epoch 789/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1419 - acc: 0.9535 - val_loss: 0.3078 - val_acc: 0.9057\n",
            "Epoch 790/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1411 - acc: 0.9511 - val_loss: 0.3495 - val_acc: 0.8885\n",
            "Epoch 791/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1523 - acc: 0.9517 - val_loss: 0.3230 - val_acc: 0.9032\n",
            "Epoch 792/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.1558 - acc: 0.9466 - val_loss: 0.3256 - val_acc: 0.8996\n",
            "Epoch 793/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1495 - acc: 0.9499 - val_loss: 0.3305 - val_acc: 0.9020\n",
            "Epoch 794/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1629 - acc: 0.9415 - val_loss: 0.3162 - val_acc: 0.9063\n",
            "Epoch 795/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1518 - acc: 0.9490 - val_loss: 0.3227 - val_acc: 0.9069\n",
            "Epoch 796/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1505 - acc: 0.9499 - val_loss: 0.3413 - val_acc: 0.8996\n",
            "Epoch 797/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1428 - acc: 0.9532 - val_loss: 0.3277 - val_acc: 0.9051\n",
            "Epoch 798/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1448 - acc: 0.9502 - val_loss: 0.3313 - val_acc: 0.9008\n",
            "Epoch 799/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1441 - acc: 0.9517 - val_loss: 0.3094 - val_acc: 0.9112\n",
            "Epoch 800/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1644 - acc: 0.9442 - val_loss: 0.3085 - val_acc: 0.9100\n",
            "Epoch 801/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1462 - acc: 0.9472 - val_loss: 0.3147 - val_acc: 0.9088\n",
            "Epoch 802/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1527 - acc: 0.9517 - val_loss: 0.3124 - val_acc: 0.9069\n",
            "Epoch 803/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1529 - acc: 0.9439 - val_loss: 0.3191 - val_acc: 0.9032\n",
            "Epoch 804/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.1478 - acc: 0.9508 - val_loss: 0.3320 - val_acc: 0.8977\n",
            "Epoch 805/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1397 - acc: 0.9520 - val_loss: 0.3666 - val_acc: 0.8904\n",
            "Epoch 806/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1385 - acc: 0.9511 - val_loss: 0.3364 - val_acc: 0.8996\n",
            "Epoch 807/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1482 - acc: 0.9484 - val_loss: 0.3160 - val_acc: 0.9039\n",
            "Epoch 808/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1491 - acc: 0.9517 - val_loss: 0.3249 - val_acc: 0.9045\n",
            "Epoch 809/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1312 - acc: 0.9560 - val_loss: 0.3314 - val_acc: 0.9069\n",
            "Epoch 810/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1412 - acc: 0.9481 - val_loss: 0.3509 - val_acc: 0.8996\n",
            "Epoch 811/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1557 - acc: 0.9478 - val_loss: 0.3358 - val_acc: 0.8971\n",
            "Epoch 812/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1590 - acc: 0.9442 - val_loss: 0.3402 - val_acc: 0.8965\n",
            "Epoch 813/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1408 - acc: 0.9526 - val_loss: 0.3389 - val_acc: 0.9039\n",
            "Epoch 814/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1385 - acc: 0.9554 - val_loss: 0.3423 - val_acc: 0.8990\n",
            "Epoch 815/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1557 - acc: 0.9472 - val_loss: 0.3017 - val_acc: 0.9124\n",
            "Epoch 816/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1438 - acc: 0.9554 - val_loss: 0.3381 - val_acc: 0.8953\n",
            "Epoch 817/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1534 - acc: 0.9466 - val_loss: 0.3156 - val_acc: 0.9057\n",
            "Epoch 818/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1435 - acc: 0.9541 - val_loss: 0.3457 - val_acc: 0.9039\n",
            "Epoch 819/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1423 - acc: 0.9575 - val_loss: 0.3178 - val_acc: 0.9088\n",
            "Epoch 820/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1296 - acc: 0.9523 - val_loss: 0.3411 - val_acc: 0.9057\n",
            "Epoch 821/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1544 - acc: 0.9460 - val_loss: 0.3337 - val_acc: 0.9051\n",
            "Epoch 822/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1320 - acc: 0.9569 - val_loss: 0.3193 - val_acc: 0.9106\n",
            "Epoch 823/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1436 - acc: 0.9535 - val_loss: 0.3067 - val_acc: 0.9081\n",
            "Epoch 824/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1384 - acc: 0.9538 - val_loss: 0.3303 - val_acc: 0.9094\n",
            "Epoch 825/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1456 - acc: 0.9499 - val_loss: 0.3479 - val_acc: 0.8928\n",
            "Epoch 826/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1450 - acc: 0.9578 - val_loss: 0.3504 - val_acc: 0.8977\n",
            "Epoch 827/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1418 - acc: 0.9481 - val_loss: 0.3198 - val_acc: 0.9106\n",
            "Epoch 828/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1438 - acc: 0.9514 - val_loss: 0.3299 - val_acc: 0.8959\n",
            "Epoch 829/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1364 - acc: 0.9544 - val_loss: 0.3407 - val_acc: 0.8971\n",
            "Epoch 830/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1462 - acc: 0.9484 - val_loss: 0.3346 - val_acc: 0.9069\n",
            "Epoch 831/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1370 - acc: 0.9532 - val_loss: 0.3315 - val_acc: 0.9045\n",
            "Epoch 832/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1331 - acc: 0.9514 - val_loss: 0.3131 - val_acc: 0.9069\n",
            "Epoch 833/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1467 - acc: 0.9484 - val_loss: 0.3139 - val_acc: 0.9081\n",
            "Epoch 834/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1342 - acc: 0.9535 - val_loss: 0.3431 - val_acc: 0.8977\n",
            "Epoch 835/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1485 - acc: 0.9475 - val_loss: 0.3101 - val_acc: 0.9088\n",
            "Epoch 836/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1273 - acc: 0.9548 - val_loss: 0.3179 - val_acc: 0.9094\n",
            "Epoch 837/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1371 - acc: 0.9587 - val_loss: 0.3033 - val_acc: 0.9088\n",
            "Epoch 838/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1240 - acc: 0.9551 - val_loss: 0.3202 - val_acc: 0.9014\n",
            "Epoch 839/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1415 - acc: 0.9472 - val_loss: 0.3405 - val_acc: 0.8977\n",
            "Epoch 840/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1414 - acc: 0.9529 - val_loss: 0.3240 - val_acc: 0.9008\n",
            "Epoch 841/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1429 - acc: 0.9532 - val_loss: 0.3292 - val_acc: 0.9063\n",
            "Epoch 842/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.1450 - acc: 0.9493 - val_loss: 0.3164 - val_acc: 0.9020\n",
            "Epoch 843/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1446 - acc: 0.9517 - val_loss: 0.3483 - val_acc: 0.9014\n",
            "Epoch 844/1000\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.1338 - acc: 0.9538 - val_loss: 0.3324 - val_acc: 0.9032\n",
            "Epoch 845/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.1424 - acc: 0.9517 - val_loss: 0.3263 - val_acc: 0.9008\n",
            "Epoch 846/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.1370 - acc: 0.9544 - val_loss: 0.3249 - val_acc: 0.9051\n",
            "Epoch 847/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.1464 - acc: 0.9487 - val_loss: 0.3119 - val_acc: 0.9192\n",
            "Epoch 848/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1361 - acc: 0.9541 - val_loss: 0.3301 - val_acc: 0.9118\n",
            "Epoch 849/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1446 - acc: 0.9469 - val_loss: 0.3365 - val_acc: 0.9075\n",
            "Epoch 850/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1238 - acc: 0.9611 - val_loss: 0.3509 - val_acc: 0.8996\n",
            "Epoch 851/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1456 - acc: 0.9535 - val_loss: 0.3283 - val_acc: 0.9075\n",
            "Epoch 852/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1201 - acc: 0.9578 - val_loss: 0.3401 - val_acc: 0.9039\n",
            "Epoch 853/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1346 - acc: 0.9535 - val_loss: 0.3209 - val_acc: 0.9063\n",
            "Epoch 854/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1431 - acc: 0.9569 - val_loss: 0.3313 - val_acc: 0.8983\n",
            "Epoch 855/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1277 - acc: 0.9599 - val_loss: 0.3478 - val_acc: 0.8971\n",
            "Epoch 856/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1358 - acc: 0.9520 - val_loss: 0.3124 - val_acc: 0.9118\n",
            "Epoch 857/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1515 - acc: 0.9505 - val_loss: 0.3267 - val_acc: 0.9069\n",
            "Epoch 858/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1333 - acc: 0.9563 - val_loss: 0.3190 - val_acc: 0.9112\n",
            "Epoch 859/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1268 - acc: 0.9569 - val_loss: 0.3295 - val_acc: 0.9045\n",
            "Epoch 860/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1298 - acc: 0.9548 - val_loss: 0.3042 - val_acc: 0.9179\n",
            "Epoch 861/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1393 - acc: 0.9535 - val_loss: 0.3282 - val_acc: 0.9008\n",
            "Epoch 862/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1350 - acc: 0.9551 - val_loss: 0.3280 - val_acc: 0.9112\n",
            "Epoch 863/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1401 - acc: 0.9523 - val_loss: 0.3420 - val_acc: 0.9002\n",
            "Epoch 864/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1236 - acc: 0.9566 - val_loss: 0.3320 - val_acc: 0.8983\n",
            "Epoch 865/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1242 - acc: 0.9587 - val_loss: 0.3185 - val_acc: 0.9081\n",
            "Epoch 866/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1412 - acc: 0.9532 - val_loss: 0.3155 - val_acc: 0.9075\n",
            "Epoch 867/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1342 - acc: 0.9587 - val_loss: 0.3204 - val_acc: 0.9075\n",
            "Epoch 868/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1362 - acc: 0.9517 - val_loss: 0.3070 - val_acc: 0.9063\n",
            "Epoch 869/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1244 - acc: 0.9566 - val_loss: 0.3484 - val_acc: 0.9020\n",
            "Epoch 870/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1430 - acc: 0.9532 - val_loss: 0.3298 - val_acc: 0.9069\n",
            "Epoch 871/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1386 - acc: 0.9523 - val_loss: 0.3401 - val_acc: 0.9008\n",
            "Epoch 872/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1241 - acc: 0.9535 - val_loss: 0.3093 - val_acc: 0.9167\n",
            "Epoch 873/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1267 - acc: 0.9578 - val_loss: 0.3192 - val_acc: 0.9155\n",
            "Epoch 874/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1371 - acc: 0.9560 - val_loss: 0.3120 - val_acc: 0.9045\n",
            "Epoch 875/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1344 - acc: 0.9532 - val_loss: 0.3242 - val_acc: 0.9026\n",
            "Epoch 876/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1407 - acc: 0.9529 - val_loss: 0.3245 - val_acc: 0.9167\n",
            "Epoch 877/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1309 - acc: 0.9511 - val_loss: 0.3346 - val_acc: 0.9008\n",
            "Epoch 878/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1204 - acc: 0.9614 - val_loss: 0.3119 - val_acc: 0.9051\n",
            "Epoch 879/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1204 - acc: 0.9569 - val_loss: 0.3277 - val_acc: 0.9069\n",
            "Epoch 880/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.1414 - acc: 0.9541 - val_loss: 0.3359 - val_acc: 0.9081\n",
            "Epoch 881/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1202 - acc: 0.9590 - val_loss: 0.3226 - val_acc: 0.9051\n",
            "Epoch 882/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1297 - acc: 0.9544 - val_loss: 0.3302 - val_acc: 0.8996\n",
            "Epoch 883/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1343 - acc: 0.9472 - val_loss: 0.3386 - val_acc: 0.9069\n",
            "Epoch 884/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.1191 - acc: 0.9566 - val_loss: 0.3091 - val_acc: 0.9124\n",
            "Epoch 885/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1360 - acc: 0.9523 - val_loss: 0.3394 - val_acc: 0.9094\n",
            "Epoch 886/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1225 - acc: 0.9581 - val_loss: 0.3128 - val_acc: 0.9057\n",
            "Epoch 887/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1407 - acc: 0.9572 - val_loss: 0.3042 - val_acc: 0.9149\n",
            "Epoch 888/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1274 - acc: 0.9541 - val_loss: 0.3370 - val_acc: 0.9002\n",
            "Epoch 889/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1235 - acc: 0.9575 - val_loss: 0.3249 - val_acc: 0.9014\n",
            "Epoch 890/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1307 - acc: 0.9557 - val_loss: 0.3217 - val_acc: 0.9032\n",
            "Epoch 891/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1154 - acc: 0.9608 - val_loss: 0.3245 - val_acc: 0.9088\n",
            "Epoch 892/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1267 - acc: 0.9569 - val_loss: 0.3213 - val_acc: 0.9081\n",
            "Epoch 893/1000\n",
            "3315/3315 [==============================] - 1s 406us/step - loss: 0.1436 - acc: 0.9511 - val_loss: 0.3568 - val_acc: 0.9057\n",
            "Epoch 894/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1345 - acc: 0.9563 - val_loss: 0.3201 - val_acc: 0.9081\n",
            "Epoch 895/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1279 - acc: 0.9560 - val_loss: 0.3553 - val_acc: 0.9081\n",
            "Epoch 896/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.1335 - acc: 0.9578 - val_loss: 0.3204 - val_acc: 0.9112\n",
            "Epoch 897/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.1333 - acc: 0.9563 - val_loss: 0.3274 - val_acc: 0.9130\n",
            "Epoch 898/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1250 - acc: 0.9596 - val_loss: 0.3251 - val_acc: 0.9130\n",
            "Epoch 899/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1154 - acc: 0.9608 - val_loss: 0.3168 - val_acc: 0.9130\n",
            "Epoch 900/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1198 - acc: 0.9626 - val_loss: 0.3085 - val_acc: 0.9167\n",
            "Epoch 901/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1301 - acc: 0.9563 - val_loss: 0.3296 - val_acc: 0.9081\n",
            "Epoch 902/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1357 - acc: 0.9566 - val_loss: 0.3331 - val_acc: 0.9088\n",
            "Epoch 903/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1304 - acc: 0.9569 - val_loss: 0.3093 - val_acc: 0.9130\n",
            "Epoch 904/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.1170 - acc: 0.9617 - val_loss: 0.3278 - val_acc: 0.9118\n",
            "Epoch 905/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1266 - acc: 0.9578 - val_loss: 0.2956 - val_acc: 0.9130\n",
            "Epoch 906/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1196 - acc: 0.9605 - val_loss: 0.3262 - val_acc: 0.9106\n",
            "Epoch 907/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1201 - acc: 0.9587 - val_loss: 0.3156 - val_acc: 0.9100\n",
            "Epoch 908/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1203 - acc: 0.9602 - val_loss: 0.3382 - val_acc: 0.9124\n",
            "Epoch 909/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1281 - acc: 0.9578 - val_loss: 0.3241 - val_acc: 0.9032\n",
            "Epoch 910/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1330 - acc: 0.9560 - val_loss: 0.3265 - val_acc: 0.9063\n",
            "Epoch 911/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1321 - acc: 0.9532 - val_loss: 0.3480 - val_acc: 0.9020\n",
            "Epoch 912/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1324 - acc: 0.9632 - val_loss: 0.3406 - val_acc: 0.9075\n",
            "Epoch 913/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1195 - acc: 0.9596 - val_loss: 0.3278 - val_acc: 0.9002\n",
            "Epoch 914/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.1224 - acc: 0.9596 - val_loss: 0.3192 - val_acc: 0.9124\n",
            "Epoch 915/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1132 - acc: 0.9617 - val_loss: 0.3060 - val_acc: 0.9081\n",
            "Epoch 916/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1165 - acc: 0.9617 - val_loss: 0.3208 - val_acc: 0.9075\n",
            "Epoch 917/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1267 - acc: 0.9575 - val_loss: 0.3110 - val_acc: 0.9137\n",
            "Epoch 918/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1154 - acc: 0.9623 - val_loss: 0.3041 - val_acc: 0.9155\n",
            "Epoch 919/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1241 - acc: 0.9584 - val_loss: 0.3270 - val_acc: 0.9081\n",
            "Epoch 920/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1322 - acc: 0.9572 - val_loss: 0.2985 - val_acc: 0.9155\n",
            "Epoch 921/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1228 - acc: 0.9581 - val_loss: 0.3257 - val_acc: 0.9081\n",
            "Epoch 922/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1096 - acc: 0.9629 - val_loss: 0.3016 - val_acc: 0.9149\n",
            "Epoch 923/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1296 - acc: 0.9505 - val_loss: 0.3331 - val_acc: 0.9075\n",
            "Epoch 924/1000\n",
            "3315/3315 [==============================] - 1s 408us/step - loss: 0.1226 - acc: 0.9599 - val_loss: 0.3274 - val_acc: 0.9075\n",
            "Epoch 925/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.1157 - acc: 0.9605 - val_loss: 0.3574 - val_acc: 0.9045\n",
            "Epoch 926/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1350 - acc: 0.9566 - val_loss: 0.3355 - val_acc: 0.9081\n",
            "Epoch 927/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1238 - acc: 0.9593 - val_loss: 0.3268 - val_acc: 0.9130\n",
            "Epoch 928/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1171 - acc: 0.9620 - val_loss: 0.3233 - val_acc: 0.9081\n",
            "Epoch 929/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1182 - acc: 0.9620 - val_loss: 0.3448 - val_acc: 0.9124\n",
            "Epoch 930/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1176 - acc: 0.9608 - val_loss: 0.3162 - val_acc: 0.9100\n",
            "Epoch 931/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1264 - acc: 0.9581 - val_loss: 0.3093 - val_acc: 0.9100\n",
            "Epoch 932/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1235 - acc: 0.9587 - val_loss: 0.3269 - val_acc: 0.9100\n",
            "Epoch 933/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1139 - acc: 0.9596 - val_loss: 0.3242 - val_acc: 0.9094\n",
            "Epoch 934/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1214 - acc: 0.9566 - val_loss: 0.3187 - val_acc: 0.9118\n",
            "Epoch 935/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1156 - acc: 0.9632 - val_loss: 0.3135 - val_acc: 0.9088\n",
            "Epoch 936/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.1303 - acc: 0.9529 - val_loss: 0.3067 - val_acc: 0.9137\n",
            "Epoch 937/1000\n",
            "3315/3315 [==============================] - 1s 405us/step - loss: 0.1185 - acc: 0.9611 - val_loss: 0.3546 - val_acc: 0.8996\n",
            "Epoch 938/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.1232 - acc: 0.9548 - val_loss: 0.3065 - val_acc: 0.9149\n",
            "Epoch 939/1000\n",
            "3315/3315 [==============================] - 1s 404us/step - loss: 0.1114 - acc: 0.9602 - val_loss: 0.3135 - val_acc: 0.9112\n",
            "Epoch 940/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1266 - acc: 0.9593 - val_loss: 0.3114 - val_acc: 0.9161\n",
            "Epoch 941/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1172 - acc: 0.9632 - val_loss: 0.3172 - val_acc: 0.9100\n",
            "Epoch 942/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.1243 - acc: 0.9584 - val_loss: 0.3248 - val_acc: 0.9118\n",
            "Epoch 943/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1171 - acc: 0.9599 - val_loss: 0.3099 - val_acc: 0.9143\n",
            "Epoch 944/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1352 - acc: 0.9569 - val_loss: 0.3032 - val_acc: 0.9173\n",
            "Epoch 945/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1174 - acc: 0.9599 - val_loss: 0.3253 - val_acc: 0.9075\n",
            "Epoch 946/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1126 - acc: 0.9602 - val_loss: 0.3242 - val_acc: 0.9081\n",
            "Epoch 947/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1309 - acc: 0.9526 - val_loss: 0.3004 - val_acc: 0.9143\n",
            "Epoch 948/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1199 - acc: 0.9596 - val_loss: 0.3107 - val_acc: 0.9057\n",
            "Epoch 949/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1124 - acc: 0.9626 - val_loss: 0.3098 - val_acc: 0.9039\n",
            "Epoch 950/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1168 - acc: 0.9620 - val_loss: 0.3048 - val_acc: 0.9106\n",
            "Epoch 951/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.1284 - acc: 0.9599 - val_loss: 0.3059 - val_acc: 0.9112\n",
            "Epoch 952/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1155 - acc: 0.9629 - val_loss: 0.3183 - val_acc: 0.9161\n",
            "Epoch 953/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1291 - acc: 0.9560 - val_loss: 0.2999 - val_acc: 0.9161\n",
            "Epoch 954/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1192 - acc: 0.9584 - val_loss: 0.3369 - val_acc: 0.9014\n",
            "Epoch 955/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1149 - acc: 0.9620 - val_loss: 0.3064 - val_acc: 0.9124\n",
            "Epoch 956/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1147 - acc: 0.9629 - val_loss: 0.3111 - val_acc: 0.9106\n",
            "Epoch 957/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1084 - acc: 0.9608 - val_loss: 0.3137 - val_acc: 0.9069\n",
            "Epoch 958/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1064 - acc: 0.9647 - val_loss: 0.3377 - val_acc: 0.9051\n",
            "Epoch 959/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1287 - acc: 0.9557 - val_loss: 0.3126 - val_acc: 0.9112\n",
            "Epoch 960/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1221 - acc: 0.9569 - val_loss: 0.3181 - val_acc: 0.9081\n",
            "Epoch 961/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1017 - acc: 0.9638 - val_loss: 0.3180 - val_acc: 0.9106\n",
            "Epoch 962/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1104 - acc: 0.9599 - val_loss: 0.2886 - val_acc: 0.9186\n",
            "Epoch 963/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1367 - acc: 0.9578 - val_loss: 0.3016 - val_acc: 0.9198\n",
            "Epoch 964/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1270 - acc: 0.9611 - val_loss: 0.2966 - val_acc: 0.9222\n",
            "Epoch 965/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.1210 - acc: 0.9572 - val_loss: 0.3100 - val_acc: 0.9118\n",
            "Epoch 966/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1138 - acc: 0.9581 - val_loss: 0.3469 - val_acc: 0.9032\n",
            "Epoch 967/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1194 - acc: 0.9599 - val_loss: 0.3358 - val_acc: 0.9118\n",
            "Epoch 968/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1071 - acc: 0.9620 - val_loss: 0.3263 - val_acc: 0.9063\n",
            "Epoch 969/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.1177 - acc: 0.9587 - val_loss: 0.3164 - val_acc: 0.9094\n",
            "Epoch 970/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.1049 - acc: 0.9644 - val_loss: 0.3308 - val_acc: 0.9143\n",
            "Epoch 971/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1075 - acc: 0.9647 - val_loss: 0.3212 - val_acc: 0.9057\n",
            "Epoch 972/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1181 - acc: 0.9611 - val_loss: 0.3337 - val_acc: 0.9063\n",
            "Epoch 973/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1094 - acc: 0.9611 - val_loss: 0.3341 - val_acc: 0.9081\n",
            "Epoch 974/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1229 - acc: 0.9602 - val_loss: 0.3340 - val_acc: 0.9026\n",
            "Epoch 975/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1187 - acc: 0.9593 - val_loss: 0.3356 - val_acc: 0.9124\n",
            "Epoch 976/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1304 - acc: 0.9544 - val_loss: 0.3273 - val_acc: 0.9124\n",
            "Epoch 977/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1202 - acc: 0.9581 - val_loss: 0.3165 - val_acc: 0.9130\n",
            "Epoch 978/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1034 - acc: 0.9695 - val_loss: 0.3060 - val_acc: 0.9118\n",
            "Epoch 979/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1195 - acc: 0.9608 - val_loss: 0.3205 - val_acc: 0.9130\n",
            "Epoch 980/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1108 - acc: 0.9605 - val_loss: 0.3116 - val_acc: 0.9112\n",
            "Epoch 981/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1187 - acc: 0.9608 - val_loss: 0.3361 - val_acc: 0.9167\n",
            "Epoch 982/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1119 - acc: 0.9611 - val_loss: 0.3198 - val_acc: 0.9112\n",
            "Epoch 983/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1057 - acc: 0.9641 - val_loss: 0.3323 - val_acc: 0.9075\n",
            "Epoch 984/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1115 - acc: 0.9611 - val_loss: 0.3251 - val_acc: 0.9069\n",
            "Epoch 985/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1049 - acc: 0.9677 - val_loss: 0.3367 - val_acc: 0.9045\n",
            "Epoch 986/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1198 - acc: 0.9575 - val_loss: 0.3080 - val_acc: 0.9155\n",
            "Epoch 987/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1204 - acc: 0.9644 - val_loss: 0.3197 - val_acc: 0.9039\n",
            "Epoch 988/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1146 - acc: 0.9614 - val_loss: 0.3177 - val_acc: 0.9106\n",
            "Epoch 989/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1212 - acc: 0.9593 - val_loss: 0.3273 - val_acc: 0.9130\n",
            "Epoch 990/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1085 - acc: 0.9632 - val_loss: 0.3281 - val_acc: 0.9149\n",
            "Epoch 991/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1120 - acc: 0.9623 - val_loss: 0.3168 - val_acc: 0.9173\n",
            "Epoch 992/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1164 - acc: 0.9614 - val_loss: 0.3072 - val_acc: 0.9149\n",
            "Epoch 993/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1143 - acc: 0.9608 - val_loss: 0.3233 - val_acc: 0.9192\n",
            "Epoch 994/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1148 - acc: 0.9554 - val_loss: 0.3448 - val_acc: 0.9106\n",
            "Epoch 995/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1175 - acc: 0.9626 - val_loss: 0.3383 - val_acc: 0.9039\n",
            "Epoch 996/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1062 - acc: 0.9641 - val_loss: 0.3111 - val_acc: 0.9106\n",
            "Epoch 997/1000\n",
            "3315/3315 [==============================] - 2s 510us/step - loss: 0.1005 - acc: 0.9695 - val_loss: 0.3135 - val_acc: 0.9130\n",
            "Epoch 998/1000\n",
            "3315/3315 [==============================] - 2s 533us/step - loss: 0.0967 - acc: 0.9680 - val_loss: 0.3133 - val_acc: 0.9137\n",
            "Epoch 999/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1057 - acc: 0.9635 - val_loss: 0.3256 - val_acc: 0.9094\n",
            "Epoch 1000/1000\n",
            "3315/3315 [==============================] - 2s 528us/step - loss: 0.1147 - acc: 0.9590 - val_loss: 0.3091 - val_acc: 0.9100\n"
          ]
        }
      ],
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mFytY6LDzgJ0"
      },
      "source": [
        "Let's plot the loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "colab_type": "code",
        "id": "TFz4ClZov9gZ",
        "outputId": "46dcf1d8-4feb-40e0-b4e6-4bc25aeb835d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f97e66d6fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vf1W7LgP2DA5"
      },
      "source": [
        "\n",
        "\n",
        "And now let's plot the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "colab_type": "code",
        "id": "8yyFBt7ASPUe",
        "outputId": "3b82ec7a-b61e-4101-a990-0b829f36d254"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f97e66796d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(cnnhistory.history['acc'])\n",
        "plt.plot(cnnhistory.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x_ySPOyHxkZ3"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "f5kRmoD-sdHj",
        "outputId": "66353b53-fd46-48b3-b3c8-550806b80f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5 \n"
          ]
        }
      ],
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MNUiznKNwUtJ"
      },
      "source": [
        "# Reloading the model to test it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "colab_type": "code",
        "id": "T4oAv6Kx8RBE",
        "outputId": "cf29c8b7-5aed-41ce-d5b5-d353137f0272"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                6410      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 89,226\n",
            "Trainable params: 89,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "loaded_model = keras.models.load_model('/content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5')\n",
        "loaded_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FHtPzc0Y8hfZ"
      },
      "source": [
        "# Checking the accuracy of the loaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "qUi-Zjuf8hDB",
        "outputId": "9dabf117-af4e-40e6-f106-99aa65c5feae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1633/1633 [==============================] - 0s 125us/step\n",
            "Restored model, accuracy: 91.00%\n"
          ]
        }
      ],
      "source": [
        "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8pXH3y7S9A1N"
      },
      "source": [
        "# Thank you for your attention! To be continued.."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "EmotionsRecognition.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
